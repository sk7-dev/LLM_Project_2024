{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input1.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  310647\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"(),-.0123456789:;?அஆஇஈஉஊஎஏஐஒஓஔகஙசஜஞடணதநனபமயரறலளழவஷஸஹாிீுூெேைொோௌ்–‘’“”…\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 53, 51, 59, 46, 54]\n",
      "காவேரி\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"காவேரி\"))\n",
    "print(decode(encode(\"காவேரி\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '\\n')\n",
      "(1, ' ')\n",
      "(2, '!')\n",
      "(3, '#')\n",
      "(4, '&')\n",
      "(5, '(')\n",
      "(6, ')')\n",
      "(7, ',')\n",
      "(8, '.')\n",
      "(9, '/')\n",
      "(10, '0')\n",
      "(11, '1')\n",
      "(12, '2')\n",
      "(13, '5')\n",
      "(14, '8')\n",
      "(15, '9')\n",
      "(16, ';')\n",
      "(17, '<')\n",
      "(18, '>')\n",
      "(19, '?')\n",
      "(20, 'e')\n",
      "(21, 'p')\n",
      "(22, 'r')\n",
      "(23, 'அ')\n",
      "(24, 'ஆ')\n",
      "(25, 'இ')\n",
      "(26, 'ஈ')\n",
      "(27, 'உ')\n",
      "(28, 'எ')\n",
      "(29, 'ஏ')\n",
      "(30, 'ஐ')\n",
      "(31, 'ஒ')\n",
      "(32, 'ஓ')\n",
      "(33, 'க')\n",
      "(34, 'ங')\n",
      "(35, 'ச')\n",
      "(36, 'ஜ')\n",
      "(37, 'ஞ')\n",
      "(38, 'ட')\n",
      "(39, 'ண')\n",
      "(40, 'த')\n",
      "(41, 'ந')\n",
      "(42, 'ன')\n",
      "(43, 'ப')\n",
      "(44, 'ம')\n",
      "(45, 'ய')\n",
      "(46, 'ர')\n",
      "(47, 'ற')\n",
      "(48, 'ல')\n",
      "(49, 'ள')\n",
      "(50, 'ழ')\n",
      "(51, 'வ')\n",
      "(52, 'ஷ')\n",
      "(53, 'ா')\n",
      "(54, 'ி')\n",
      "(55, 'ீ')\n",
      "(56, 'ு')\n",
      "(57, 'ூ')\n",
      "(58, 'ெ')\n",
      "(59, 'ே')\n",
      "(60, 'ை')\n",
      "(61, 'ொ')\n",
      "(62, 'ோ')\n",
      "(63, 'ௌ')\n",
      "(64, '்')\n",
      "(65, '–')\n",
      "(66, '‘')\n",
      "(67, '’')\n",
      "(68, '“')\n",
      "(69, '”')\n"
     ]
    }
   ],
   "source": [
    "for i in enumerate(chars):\n",
    "    print(i) #it will return a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(21, 'I'), (28, 'P'), (50, 'l'), (8, '.'), (51, 'm'), (5, \"'\"), (20, 'H'), (42, 'd'), (62, 'x'), (44, 'f'), (30, 'R'), (35, 'W'), (7, '-'), (29, 'Q'), (27, 'O'), (59, 'u'), (3, '$'), (34, 'V'), (12, '?'), (1, ' '), (46, 'h'), (58, 't'), (18, 'F'), (48, 'j'), (15, 'C'), (13, 'A'), (22, 'J'), (10, ':'), (57, 's'), (2, '!'), (61, 'w'), (41, 'c'), (63, 'y'), (24, 'L'), (53, 'o'), (56, 'r'), (36, 'X'), (43, 'e'), (47, 'i'), (39, 'a'), (16, 'D'), (52, 'n'), (25, 'M'), (23, 'K'), (32, 'T'), (37, 'Y'), (49, 'k'), (40, 'b'), (11, ';'), (14, 'B'), (60, 'v'), (17, 'E'), (38, 'Z'), (4, '&'), (19, 'G'), (31, 'S'), (54, 'p'), (64, 'z'), (0, '\\n'), (6, ','), (26, 'N'), (45, 'g'), (9, '3'), (55, 'q'), (33, 'U')}\n"
     ]
    }
   ],
   "source": [
    "stoi = { i for i in enumerate(chars) }\n",
    "print(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "print(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"lambda\" functions are anonymous functions. They are useful when you need a small function temporarily and don't want to define a full function with def.** `lambda parameters: expression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "square = lambda x: x ** 2\n",
    "print(square(5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39, 40, 41]\n"
     ]
    }
   ],
   "source": [
    "encode = lambda s: [stoi[c] for c in s] #list comprehension\n",
    "a = encode(\"abc\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39, 40, 41]\n"
     ]
    }
   ],
   "source": [
    "s = 'abc'\n",
    "def encode(s): \n",
    "    a = [] \n",
    "    for c in s:  \n",
    "        a.append(stoi[c])\n",
    "    return a\n",
    "data = encode(s)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PyTorch tensor is a multi-dimensional array or matrix that is used to store data and perform computations in PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0130,  0.4786, -1.5338],\n",
      "        [-0.5835,  1.9606, -2.1773],\n",
      "        [-1.2816, -1.5758, -0.3407]])\n"
     ]
    }
   ],
   "source": [
    "random_tensor = torch.randn(3, 3)  # 3x3 tensor with random values from normal distribution\n",
    "print(random_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([32, 8])\n",
      "tensor([[42, 56, 39, 47, 52,  0, 32, 46],\n",
      "        [56, 51, 43, 56,  1, 51, 39, 50],\n",
      "        [17, 31, 32, 25, 27, 30, 17, 24],\n",
      "        [26, 16, 10,  0, 32, 46, 43,  1],\n",
      "        [ 0, 32, 46, 43,  1, 54, 59, 56],\n",
      "        [42,  1, 41, 46, 53, 49, 43,  1],\n",
      "        [17, 26, 32, 21, 27, 10,  0, 31],\n",
      "        [51, 63,  1, 57, 39, 63, 47, 52],\n",
      "        [59,  1, 44, 39, 50, 50,  1, 59],\n",
      "        [56,  1, 43, 56, 43,  1, 63, 53],\n",
      "        [43,  1, 43, 60, 43, 56, 63,  1],\n",
      "        [ 1, 57, 43, 52, 42, 47, 52, 45],\n",
      "        [ 1, 57, 43, 56, 60, 39, 52, 58],\n",
      "        [46, 53, 59,  1, 41, 39, 52, 57],\n",
      "        [50, 58,  1, 40, 43,  1, 53, 52],\n",
      "        [46,  1, 40, 63,  1, 51, 63,  1],\n",
      "        [56, 43, 57, 53, 50, 60, 43, 42],\n",
      "        [ 1, 42, 47, 42,  0, 30, 59, 52],\n",
      "        [58, 58, 43, 56,  1, 57, 43, 52],\n",
      "        [42, 43, 56,  6,  1, 63, 53, 59],\n",
      "        [50,  1, 39, 57,  1, 50, 53, 52],\n",
      "        [50, 53, 61, 43, 56, 57,  1, 58],\n",
      "        [39, 56,  6,  1, 61, 46, 53,  1],\n",
      "        [53, 60, 43,  1, 44, 50, 39, 58],\n",
      "        [39, 57,  1, 52, 53, 58,  1, 58],\n",
      "        [ 7, 47, 52,  1, 58, 46, 43,  1],\n",
      "        [31, 47, 41, 47, 50, 47, 39,  6],\n",
      "        [43, 47, 56,  1, 53, 44, 44, 47],\n",
      "        [11,  0, 31, 59, 54, 54, 43, 56],\n",
      "        [39, 57, 43, 42,  1, 63, 53, 59],\n",
      "        [ 1, 53,  5, 43, 56,  1, 53, 59],\n",
      "        [39, 58, 46, 43, 56,  6,  1, 58]])\n",
      "targets:\n",
      "torch.Size([32, 8])\n",
      "tensor([[56, 39, 47, 52,  0, 32, 46, 43],\n",
      "        [51, 43, 56,  1, 51, 39, 50, 39],\n",
      "        [31, 32, 25, 27, 30, 17, 24, 13],\n",
      "        [16, 10,  0, 32, 46, 43,  1, 61],\n",
      "        [32, 46, 43,  1, 54, 59, 56, 47],\n",
      "        [ 1, 41, 46, 53, 49, 43,  1, 63],\n",
      "        [26, 32, 21, 27, 10,  0, 31, 46],\n",
      "        [63,  1, 57, 39, 63, 47, 52, 45],\n",
      "        [ 1, 44, 39, 50, 50,  1, 59, 54],\n",
      "        [ 1, 43, 56, 43,  1, 63, 53, 59],\n",
      "        [ 1, 43, 60, 43, 56, 63,  1, 53],\n",
      "        [57, 43, 52, 42, 47, 52, 45,  1],\n",
      "        [57, 43, 56, 60, 39, 52, 58, 57],\n",
      "        [53, 59,  1, 41, 39, 52, 57, 58],\n",
      "        [58,  1, 40, 43,  1, 53, 52,  1],\n",
      "        [ 1, 40, 63,  1, 51, 63,  1, 54],\n",
      "        [43, 57, 53, 50, 60, 43, 42,  0],\n",
      "        [42, 47, 42,  0, 30, 59, 52,  1],\n",
      "        [58, 43, 56,  1, 57, 43, 52, 58],\n",
      "        [43, 56,  6,  1, 63, 53, 59, 56],\n",
      "        [ 1, 39, 57,  1, 50, 53, 52, 45],\n",
      "        [53, 61, 43, 56, 57,  1, 58, 53],\n",
      "        [56,  6,  1, 61, 46, 53,  1, 46],\n",
      "        [60, 43,  1, 44, 50, 39, 58, 58],\n",
      "        [57,  1, 52, 53, 58,  1, 58, 46],\n",
      "        [47, 52,  1, 58, 46, 43,  1, 54],\n",
      "        [47, 41, 47, 50, 47, 39,  6,  0],\n",
      "        [47, 56,  1, 53, 44, 44, 47, 41],\n",
      "        [ 0, 31, 59, 54, 54, 43, 56,  1],\n",
      "        [57, 43, 42,  1, 63, 53, 59,  1],\n",
      "        [53,  5, 43, 56,  1, 53, 59, 56],\n",
      "        [58, 46, 43, 56,  6,  1, 58, 53]])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train') #comment\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['emma',\n",
    " 'olivia',\n",
    " 'ava',\n",
    " 'isabella',\n",
    " 'sophia',\n",
    " 'charlotte',\n",
    " 'mia',\n",
    " 'amelia',\n",
    " 'harper',\n",
    " 'evelyn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e m\n",
      "m m\n",
      "m a\n"
     ]
    }
   ],
   "source": [
    "for w in words[:1]:\n",
    "  for ch1, ch2 in zip(w, w[1:]):\n",
    "    print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dictionary.get(keyname, value)` where keyname is The keyname of the item you want to return the value from and value is A value to return if the specified key does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('h', 'e'): 1}\n"
     ]
    }
   ],
   "source": [
    "b = {}\n",
    "bigram = ('h', 'e')\n",
    "\n",
    "# First encounter of ('h', 'e')\n",
    "count = b.get(bigram, 0)  # Since ('h', 'e') is not in b, this returns 0\n",
    "b[bigram] = count + 1     # Sets b[('h', 'e')] to 1\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b = {}\n",
    "for w in words:\n",
    "  chs = ['<S>'] + list(w) + ['<E>']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    bigram = (ch1, ch2)\n",
    "    b[bigram] = b.get(bigram, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('a', '<E>'), 7),\n",
       " (('i', 'a'), 4),\n",
       " (('e', 'l'), 3),\n",
       " (('<S>', 'e'), 2),\n",
       " (('l', 'i'), 2),\n",
       " (('<S>', 'a'), 2),\n",
       " (('h', 'a'), 2),\n",
       " (('a', 'r'), 2),\n",
       " (('e', 'm'), 1),\n",
       " (('m', 'm'), 1),\n",
       " (('m', 'a'), 1),\n",
       " (('<S>', 'o'), 1),\n",
       " (('o', 'l'), 1),\n",
       " (('i', 'v'), 1),\n",
       " (('v', 'i'), 1),\n",
       " (('a', 'v'), 1),\n",
       " (('v', 'a'), 1),\n",
       " (('<S>', 'i'), 1),\n",
       " (('i', 's'), 1),\n",
       " (('s', 'a'), 1),\n",
       " (('a', 'b'), 1),\n",
       " (('b', 'e'), 1),\n",
       " (('l', 'l'), 1),\n",
       " (('l', 'a'), 1),\n",
       " (('<S>', 's'), 1),\n",
       " (('s', 'o'), 1),\n",
       " (('o', 'p'), 1),\n",
       " (('p', 'h'), 1),\n",
       " (('h', 'i'), 1),\n",
       " (('<S>', 'c'), 1),\n",
       " (('c', 'h'), 1),\n",
       " (('r', 'l'), 1),\n",
       " (('l', 'o'), 1),\n",
       " (('o', 't'), 1),\n",
       " (('t', 't'), 1),\n",
       " (('t', 'e'), 1),\n",
       " (('e', '<E>'), 1),\n",
       " (('<S>', 'm'), 1),\n",
       " (('m', 'i'), 1),\n",
       " (('a', 'm'), 1),\n",
       " (('m', 'e'), 1),\n",
       " (('<S>', 'h'), 1),\n",
       " (('r', 'p'), 1),\n",
       " (('p', 'e'), 1),\n",
       " (('e', 'r'), 1),\n",
       " (('r', '<E>'), 1),\n",
       " (('e', 'v'), 1),\n",
       " (('v', 'e'), 1),\n",
       " (('l', 'y'), 1),\n",
       " (('y', 'n'), 1),\n",
       " (('n', '<E>'), 1)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(b.items(), key = lambda kv: -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {\n",
    "  ('h', 'e'): 2,\n",
    "  ('e', 'l'): 1,\n",
    "  ('l', 'l'): 1,\n",
    "  ('l', 'o'): 1,\n",
    "  ('o', '<E>'): 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('h', 'e'), 2),\n",
       " (('e', 'l'), 1),\n",
       " (('l', 'l'), 1),\n",
       " (('l', 'o'), 1),\n",
       " (('o', '<E>'), 1)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(x.items(), key = lambda kv: -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"I\", \"like\", \"cats\", \"dogs\", \"car\", \"shwetha\"]\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input idx:\n",
      " tensor([[4, 4, 0, 1, 3],\n",
      "        [4, 2, 0, 5, 4]])\n",
      "\n",
      "Logits:\n",
      " tensor([[[ 1.3940, -0.5681, -0.5765, -0.0823,  0.9343, -0.6740],\n",
      "         [ 1.3940, -0.5681, -0.5765, -0.0823,  0.9343, -0.6740],\n",
      "         [-0.8982, -0.1593, -0.1351, -1.8574, -1.2355,  0.3585],\n",
      "         [-1.4117, -0.0254, -0.2865, -0.0451,  0.2312, -0.9341],\n",
      "         [ 1.0827, -0.9749, -0.1530,  0.1494,  0.7607,  0.7706]],\n",
      "\n",
      "        [[ 1.3940, -0.5681, -0.5765, -0.0823,  0.9343, -0.6740],\n",
      "         [ 0.1232, -0.8815, -0.0243,  0.6466, -1.0736, -1.4312],\n",
      "         [-0.8982, -0.1593, -0.1351, -1.8574, -1.2355,  0.3585],\n",
      "         [-0.2351,  0.6542, -3.1659,  0.2164,  1.2839,  1.4845],\n",
      "         [ 1.3940, -0.5681, -0.5765, -0.0823,  0.9343, -0.6740]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the vocabulary size and the input tensor\n",
    "vocab_size = len(vocab) # for example, a vocabulary size of 10\n",
    "B, T = 2, 5  # Batch size (B) and sequence length (T)\n",
    "\n",
    "# Initialize the embedding layer (equivalent to self.token_embedding_table)\n",
    "token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "# Define an example input tensor idx of shape (B, T)\n",
    "idx = torch.randint(0, vocab_size, (B, T))  # Random integer values in the range [0, vocab_size)\n",
    "print(\"Input idx:\\n\", idx)\n",
    "\n",
    "# Forward pass\n",
    "logits = token_embedding_table(idx)  # Output shape will be (B, T, C) where C = vocab_size\n",
    "print(\"\\nLogits:\\n\", logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits after embedding lookup: tensor([[[-0.5698, -0.3917, -2.5088, -0.2929, -0.4578],\n",
      "         [-1.7342,  0.2785, -0.7114,  1.6899,  0.5533],\n",
      "         [-0.7548,  0.2590,  1.8964, -0.0588,  0.2411]],\n",
      "\n",
      "        [[-0.7548,  0.2590,  1.8964, -0.0588,  0.2411],\n",
      "         [-0.9542, -0.0894, -1.8531, -0.1106, -0.7076],\n",
      "         [ 0.0239, -1.0283, -0.2174,  1.8099,  1.5296]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Logits shape: torch.Size([2, 3, 5])\n",
      "B, T, C: 2 3 5\n",
      "Logits after reshaping: tensor([[-0.5698, -0.3917, -2.5088, -0.2929, -0.4578],\n",
      "        [-1.7342,  0.2785, -0.7114,  1.6899,  0.5533],\n",
      "        [-0.7548,  0.2590,  1.8964, -0.0588,  0.2411],\n",
      "        [-0.7548,  0.2590,  1.8964, -0.0588,  0.2411],\n",
      "        [-0.9542, -0.0894, -1.8531, -0.1106, -0.7076],\n",
      "        [ 0.0239, -1.0283, -0.2174,  1.8099,  1.5296]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Logits shape after reshaping: torch.Size([6, 5])\n",
      "Targets after reshaping: tensor([1, 2, 3, 3, 4, 0])\n",
      "Targets shape after reshaping: torch.Size([6])\n",
      "Cross-entropy loss: tensor(2.2408, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example parameters\n",
    "B, T, vocab_size = 2, 3, 5  # batch size, sequence length, vocabulary size\n",
    "\n",
    "# Initialize embedding layer manually (usually in __init__ in a class)\n",
    "token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "# Example input tensors\n",
    "idx = torch.tensor([[0, 1, 2], [2, 3, 4]])  # (B, T)\n",
    "targets = torch.tensor([[1, 2, 3], [3, 4, 0]])  # (B, T)\n",
    "\n",
    "# Step 1: Look up logits from the embedding table\n",
    "logits = token_embedding_table(idx)  # Shape will be (B, T, C)\n",
    "print(\"Logits after embedding lookup:\", logits)\n",
    "print(\"Logits shape:\", logits.shape)  # Expected shape: (B, T, vocab_size)\n",
    "\n",
    "# Step 2: Check if targets are provided to calculate loss\n",
    "if targets is None:\n",
    "    loss = None\n",
    "else:\n",
    "    # Step 3: Get shapes of B, T, and C from logits\n",
    "    B, T, C = logits.shape\n",
    "    print(\"B, T, C:\", B, T, C)\n",
    "\n",
    "    # Step 4: Flatten logits and targets for cross-entropy computation\n",
    "    logits = logits.view(B * T, C)\n",
    "    print(\"Logits after reshaping:\", logits)\n",
    "    print(\"Logits shape after reshaping:\", logits.shape)  # Expected shape: (B*T, C)\n",
    "\n",
    "    targets = targets.view(B * T)\n",
    "    print(\"Targets after reshaping:\", targets)\n",
    "    print(\"Targets shape after reshaping:\", targets.shape)  # Expected shape: (B*T,)\n",
    "\n",
    "    # Step 5: Calculate the cross-entropy loss\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "    print(\"Cross-entropy loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`generate` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities after softmax - Dimension = -1: tensor([[0.2117, 0.6345, 0.1538],\n",
      "        [0.2117, 0.6345, 0.1538]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the vocabulary size and initialize the model\n",
    "vocab_size = 3 # Example vocabulary size\n",
    "B, T = 2, 3  # Batch size and sequence length for demonstration\n",
    "max_new_tokens = 5  # Number of new tokens to generate\n",
    "\n",
    "# Starting sequence, a (B, T) tensor of indices\n",
    "idx = torch.randint(0, vocab_size, (B, T))  # Initial random tokens\n",
    "#print(\"Initial idx:\", idx)  # Shape: (B, T)\n",
    "\n",
    "# Simulate the model's token embedding table (simplified for this example)\n",
    "token_embedding_table = torch.nn.Embedding(vocab_size, vocab_size)\n",
    "#print(\"Token Embedding Table:\", token_embedding_table.weight)\n",
    "\n",
    "# Run the generation loop for a specified number of new tokens\n",
    "for _ in range(max_new_tokens):\n",
    "    # Get the predictions by looking up embeddings (simulates the forward pass of the model)\n",
    "    logits = token_embedding_table(idx)  # Shape: (B, T, vocab_size)\n",
    "    #print(\"\\nLogits (output of token embedding):\", logits)\n",
    "\n",
    "    # Focus only on the last time step\n",
    "    logits = logits[:, -1, :]  # Shape: (B, vocab_size)\n",
    "    #print(\"Logits at the last time step:\", logits)\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    probs = F.softmax(logits, dim=-1)  # Shape: (B, vocab_size)\n",
    "    #print(\"Probabilities after softmax - Dimension = -1:\", probs)\n",
    "\n",
    "    # Sample from the distribution\n",
    "    idx_next = torch.multinomial(probs, num_samples=1)  # Shape: (B, 1)\n",
    "    #print(\"Sampled next token indices:\", idx_next)\n",
    "\n",
    "    # Append sampled index to the running sequence\n",
    "    idx = torch.cat((idx, idx_next), dim=1)  # Shape: (B, T+1)\n",
    "    #print(\"Updated idx with new token:\", idx)\n",
    "\n",
    "#print(\"\\nFinal generated sequence:\", idx)\n",
    "print(\"Probabilities after softmax - Dimension = -1:\", probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`logits_last_token = logits[:, -1, :]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original logits shape: torch.Size([3, 5, 5])\n",
      "Original logits: tensor([[[ 0.7197, -0.0312,  0.3210, -0.5477, -0.4302],\n",
      "         [ 0.7591, -1.2154, -0.3034, -0.4355,  0.2557],\n",
      "         [-1.6738,  1.3958,  0.4602,  1.1874, -0.6585],\n",
      "         [-0.2604,  0.6578, -0.6115,  0.8724, -0.7121],\n",
      "         [ 0.7083, -0.5542,  0.8778,  0.0765,  0.4233]],\n",
      "\n",
      "        [[-0.0604,  1.3901,  0.5978, -0.8579,  0.5132],\n",
      "         [ 0.3595,  1.1219,  0.0804,  0.3566,  0.1255],\n",
      "         [ 0.3984, -0.8485, -0.5143,  0.7305, -0.1207],\n",
      "         [ 1.2420,  0.9079,  0.3548,  0.7696, -0.1712],\n",
      "         [-0.4675, -0.8515,  0.0537,  0.7609, -0.6885]],\n",
      "\n",
      "        [[-1.7235,  0.6089,  1.2678,  1.1129,  0.3321],\n",
      "         [-1.3550, -1.3064,  0.0601, -1.8122,  0.6211],\n",
      "         [ 0.4274,  0.1511,  0.9602, -0.5113, -0.4914],\n",
      "         [-0.2934,  0.0562,  0.3438, -1.1166,  0.9537],\n",
      "         [-0.5646, -0.0401, -0.7710,  2.2142, -0.7124]]])\n",
      "\n",
      "Logits after selecting the last token of each sequence:\n",
      "Shape of logits_last_token: torch.Size([3, 5])\n",
      "tensor([[ 0.7083, -0.5542,  0.8778,  0.0765,  0.4233],\n",
      "        [-0.4675, -0.8515,  0.0537,  0.7609, -0.6885],\n",
      "        [-0.5646, -0.0401, -0.7710,  2.2142, -0.7124]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Let's assume we have logits of shape (B, T, C)\n",
    "B, T, C = 3, 5, 5  # 3 sequences, each of length 5, with 10 classes (vocab size)\n",
    "\n",
    "# Random logits for demonstration\n",
    "logits = torch.randn(B, T, C)\n",
    "print(\"Original logits shape:\", logits.shape)\n",
    "print(\"Original logits:\", logits)\n",
    "\n",
    "# Now apply the slicing to focus on the last time step of each sequence\n",
    "logits_last_token = logits[:, -1, :]  # Take the last token (T = 5, last one is T=-1)\n",
    "\n",
    "print(\"\\nLogits after selecting the last token of each sequence:\")\n",
    "print(\"Shape of logits_last_token:\", logits_last_token.shape)\n",
    "print(logits_last_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token indices sampled for each sequence: tensor([[2],\n",
      "        [0],\n",
      "        [0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Sample probabilities for 3 sequences (batch size = 3) with 5 possible tokens (vocabulary size = 5)\n",
    "probs = torch.tensor([[0.1, 0.3, 0.4, 0.1, 0.1],    # probabilities for sequence 1\n",
    "                      [0.2, 0.1, 0.1, 0.3, 0.3],    # probabilities for sequence 2\n",
    "                      [0.3, 0.2, 0.2, 0.1, 0.2]])   # probabilities for sequence 3\n",
    "\n",
    "# Sample the next token (1 token for each sequence)\n",
    "idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "print(\"Next token indices sampled for each sequence:\", idx_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Token', 'ization', 'Ġis', 'Ġfun', '!']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# Use GPT2TokenizerFast directly for compatibility\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "text = \"Tokenization is fun!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Byte-Pair Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karthee\\Desktop\\CSULB\\Project\\LLM Project\\llmvenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m itos \u001b[38;5;241m=\u001b[39m {i: ch \u001b[38;5;28;01mfor\u001b[39;00m i, ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chars)}  \u001b[38;5;66;03m# Integer to string\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# GPT-2 Tokenizer (subword-level encoding)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2TokenizerFast\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Use GPT2TokenizerFast directly for compatibility\u001b[39;00m\n\u001b[0;32m     11\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m GPT2TokenizerFast\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\karthee\\Desktop\\CSULB\\Project\\LLM Project\\llmvenv\\Lib\\site-packages\\transformers\\__init__.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     29\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     logging,\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     52\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\karthee\\Desktop\\CSULB\\Project\\LLM Project\\llmvenv\\Lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\karthee\\Desktop\\CSULB\\Project\\LLM Project\\llmvenv\\Lib\\site-packages\\transformers\\utils\\__init__.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lru_cache\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FrozenSet\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_full_repo_name  \u001b[38;5;66;03m# for backward compatibility\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HF_HUB_DISABLE_TELEMETRY \u001b[38;5;28;01mas\u001b[39;00m DISABLE_TELEMETRY  \u001b[38;5;66;03m# for backward compatibility\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\karthee\\Desktop\\CSULB\\Project\\LLM Project\\llmvenv\\Lib\\site-packages\\huggingface_hub\\__init__.py:547\u001b[0m, in \u001b[0;36m_attach.<locals>.__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    545\u001b[0m submod_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_to_modules[name]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 547\u001b[0m     submod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubmod_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError importing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubmod_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\karthee\\Desktop\\CSULB\\Project\\LLM Project\\llmvenv\\Lib\\site-packages\\huggingface_hub\\hf_api.py:52\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcurrent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_commit_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     53\u001b[0m     CommitOperation,\n\u001b[0;32m     54\u001b[0m     CommitOperationAdd,\n\u001b[0;32m     55\u001b[0m     CommitOperationCopy,\n\u001b[0;32m     56\u001b[0m     CommitOperationDelete,\n\u001b[0;32m     57\u001b[0m     _fetch_files_to_copy,\n\u001b[0;32m     58\u001b[0m     _fetch_upload_modes,\n\u001b[0;32m     59\u001b[0m     _prepare_commit_payload,\n\u001b[0;32m     60\u001b[0m     _upload_lfs_files,\n\u001b[0;32m     61\u001b[0m     _warn_on_overwriting_operations,\n\u001b[0;32m     62\u001b[0m )\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inference_endpoints\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InferenceEndpoint, InferenceEndpointType\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multi_commits\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     65\u001b[0m     MULTI_COMMIT_PR_CLOSE_COMMENT_FAILURE_BAD_REQUEST_TEMPLATE,\n\u001b[0;32m     66\u001b[0m     MULTI_COMMIT_PR_CLOSE_COMMENT_FAILURE_NO_CHANGES_TEMPLATE,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     75\u001b[0m     plan_multi_commits,\n\u001b[0;32m     76\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\karthee\\Desktop\\CSULB\\Project\\LLM Project\\llmvenv\\Lib\\site-packages\\huggingface_hub\\_commit_api.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EntryNotFoundError\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_download\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hf_hub_url\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlfs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UploadInfo, lfs_upload, post_lfs_batch_info\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     FORBIDDEN_FOLDERS,\n\u001b[0;32m     24\u001b[0m     chunk_iterable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     validate_hf_hub_args,\n\u001b[0;32m     31\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\karthee\\Desktop\\CSULB\\Project\\LLM Project\\llmvenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:23\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     __version__,  \u001b[38;5;66;03m# noqa: F401 # for backward compatibility\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     constants,\n\u001b[0;32m     22\u001b[0m )\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_local_folder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_local_download_paths, read_download_metadata, write_download_metadata\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     25\u001b[0m     HUGGINGFACE_CO_URL_TEMPLATE,  \u001b[38;5;66;03m# noqa: F401 # for backward compatibility\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     HUGGINGFACE_HUB_CACHE,  \u001b[38;5;66;03m# noqa: F401 # for backward compatibility\u001b[39;00m\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     29\u001b[0m     EntryNotFoundError,\n\u001b[0;32m     30\u001b[0m     FileMetadataError,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     RevisionNotFoundError,\n\u001b[0;32m     35\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\karthee\\Desktop\\CSULB\\Project\\LLM Project\\llmvenv\\Lib\\site-packages\\huggingface_hub\\_local_folder.py:60\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WeakFileLock\n\u001b[0;32m     63\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLocalDownloadFilePaths\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\karthee\\Desktop\\CSULB\\Project\\LLM Project\\llmvenv\\Lib\\site-packages\\huggingface_hub\\utils\\__init__.py:40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_auth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_stored_tokens, get_token\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_cache_assets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cached_assets_path\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_cache_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     41\u001b[0m     CachedFileInfo,\n\u001b[0;32m     42\u001b[0m     CachedRepoInfo,\n\u001b[0;32m     43\u001b[0m     CachedRevisionInfo,\n\u001b[0;32m     44\u001b[0m     DeleteCacheStrategy,\n\u001b[0;32m     45\u001b[0m     HFCacheInfo,\n\u001b[0;32m     46\u001b[0m     scan_cache_dir,\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunk_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m chunk_iterable\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_datetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_datetime\n",
      "File \u001b[1;32mc:\\Users\\karthee\\Desktop\\CSULB\\Project\\LLM Project\\llmvenv\\Lib\\site-packages\\huggingface_hub\\utils\\_cache_manager.py:27\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, FrozenSet, List, Literal, Optional, Set, Union\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CacheNotFound, CorruptedCacheException\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommands\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_cli_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tabulate\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HF_HUB_CACHE\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:991\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1087\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1186\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text = \"I love you shwetha. I miss you so much\"\n",
    "\n",
    "chars = sorted(set(text))  # Unique characters in the text\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}  # String to integer\n",
    "itos = {i: ch for i, ch in enumerate(chars)}  # Integer to string\n",
    "\n",
    "# GPT-2 Tokenizer (subword-level encoding)\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# Use GPT2TokenizerFast directly for compatibility\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokens = tokenizer.tokenize(text)\n",
    "encoded_tokens = tokenizer.encode(text)\n",
    "decoded_text = tokenizer.decode(encoded_tokens)\n",
    "\n",
    "print(\"Subword tokens:\", tokens)\n",
    "print(\"Subword encoded:\", encoded_tokens)\n",
    "print(\"Subword decoded:\", decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
