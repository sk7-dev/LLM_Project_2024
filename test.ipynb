{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input1.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1681475\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"#&'()*,-./0123456789:;<>?divஅஆஇஈஉஊஎஏஐஒஓஔகஙசஜஞடணதநனபமயரறலளழவஷஸஹாிீுூெேைொோௌ்–—‘’“”…\n",
      "85\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 53, 51, 59, 46, 54]\n",
      "காவேரி\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"காவேரி\"))\n",
    "print(decode(encode(\"காவேரி\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '\\n')\n",
      "(1, ' ')\n",
      "(2, '!')\n",
      "(3, '#')\n",
      "(4, '&')\n",
      "(5, '(')\n",
      "(6, ')')\n",
      "(7, ',')\n",
      "(8, '.')\n",
      "(9, '/')\n",
      "(10, '0')\n",
      "(11, '1')\n",
      "(12, '2')\n",
      "(13, '5')\n",
      "(14, '8')\n",
      "(15, '9')\n",
      "(16, ';')\n",
      "(17, '<')\n",
      "(18, '>')\n",
      "(19, '?')\n",
      "(20, 'e')\n",
      "(21, 'p')\n",
      "(22, 'r')\n",
      "(23, 'அ')\n",
      "(24, 'ஆ')\n",
      "(25, 'இ')\n",
      "(26, 'ஈ')\n",
      "(27, 'உ')\n",
      "(28, 'எ')\n",
      "(29, 'ஏ')\n",
      "(30, 'ஐ')\n",
      "(31, 'ஒ')\n",
      "(32, 'ஓ')\n",
      "(33, 'க')\n",
      "(34, 'ங')\n",
      "(35, 'ச')\n",
      "(36, 'ஜ')\n",
      "(37, 'ஞ')\n",
      "(38, 'ட')\n",
      "(39, 'ண')\n",
      "(40, 'த')\n",
      "(41, 'ந')\n",
      "(42, 'ன')\n",
      "(43, 'ப')\n",
      "(44, 'ம')\n",
      "(45, 'ய')\n",
      "(46, 'ர')\n",
      "(47, 'ற')\n",
      "(48, 'ல')\n",
      "(49, 'ள')\n",
      "(50, 'ழ')\n",
      "(51, 'வ')\n",
      "(52, 'ஷ')\n",
      "(53, 'ா')\n",
      "(54, 'ி')\n",
      "(55, 'ீ')\n",
      "(56, 'ு')\n",
      "(57, 'ூ')\n",
      "(58, 'ெ')\n",
      "(59, 'ே')\n",
      "(60, 'ை')\n",
      "(61, 'ொ')\n",
      "(62, 'ோ')\n",
      "(63, 'ௌ')\n",
      "(64, '்')\n",
      "(65, '–')\n",
      "(66, '‘')\n",
      "(67, '’')\n",
      "(68, '“')\n",
      "(69, '”')\n"
     ]
    }
   ],
   "source": [
    "for i in enumerate(chars):\n",
    "    print(i) #it will return a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(21, 'I'), (28, 'P'), (50, 'l'), (8, '.'), (51, 'm'), (5, \"'\"), (20, 'H'), (42, 'd'), (62, 'x'), (44, 'f'), (30, 'R'), (35, 'W'), (7, '-'), (29, 'Q'), (27, 'O'), (59, 'u'), (3, '$'), (34, 'V'), (12, '?'), (1, ' '), (46, 'h'), (58, 't'), (18, 'F'), (48, 'j'), (15, 'C'), (13, 'A'), (22, 'J'), (10, ':'), (57, 's'), (2, '!'), (61, 'w'), (41, 'c'), (63, 'y'), (24, 'L'), (53, 'o'), (56, 'r'), (36, 'X'), (43, 'e'), (47, 'i'), (39, 'a'), (16, 'D'), (52, 'n'), (25, 'M'), (23, 'K'), (32, 'T'), (37, 'Y'), (49, 'k'), (40, 'b'), (11, ';'), (14, 'B'), (60, 'v'), (17, 'E'), (38, 'Z'), (4, '&'), (19, 'G'), (31, 'S'), (54, 'p'), (64, 'z'), (0, '\\n'), (6, ','), (26, 'N'), (45, 'g'), (9, '3'), (55, 'q'), (33, 'U')}\n"
     ]
    }
   ],
   "source": [
    "stoi = { i for i in enumerate(chars) }\n",
    "print(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "print(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"lambda\" functions are anonymous functions. They are useful when you need a small function temporarily and don't want to define a full function with def.** `lambda parameters: expression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "square = lambda x: x ** 2\n",
    "print(square(5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39, 40, 41]\n"
     ]
    }
   ],
   "source": [
    "encode = lambda s: [stoi[c] for c in s] #list comprehension\n",
    "a = encode(\"abc\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39, 40, 41]\n"
     ]
    }
   ],
   "source": [
    "s = 'abc'\n",
    "def encode(s): \n",
    "    a = [] \n",
    "    for c in s:  \n",
    "        a.append(stoi[c])\n",
    "    return a\n",
    "data = encode(s)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PyTorch tensor is a multi-dimensional array or matrix that is used to store data and perform computations in PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0130,  0.4786, -1.5338],\n",
      "        [-0.5835,  1.9606, -2.1773],\n",
      "        [-1.2816, -1.5758, -0.3407]])\n"
     ]
    }
   ],
   "source": [
    "random_tensor = torch.randn(3, 3)  # 3x3 tensor with random values from normal distribution\n",
    "print(random_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([32, 8])\n",
      "tensor([[42, 56, 39, 47, 52,  0, 32, 46],\n",
      "        [56, 51, 43, 56,  1, 51, 39, 50],\n",
      "        [17, 31, 32, 25, 27, 30, 17, 24],\n",
      "        [26, 16, 10,  0, 32, 46, 43,  1],\n",
      "        [ 0, 32, 46, 43,  1, 54, 59, 56],\n",
      "        [42,  1, 41, 46, 53, 49, 43,  1],\n",
      "        [17, 26, 32, 21, 27, 10,  0, 31],\n",
      "        [51, 63,  1, 57, 39, 63, 47, 52],\n",
      "        [59,  1, 44, 39, 50, 50,  1, 59],\n",
      "        [56,  1, 43, 56, 43,  1, 63, 53],\n",
      "        [43,  1, 43, 60, 43, 56, 63,  1],\n",
      "        [ 1, 57, 43, 52, 42, 47, 52, 45],\n",
      "        [ 1, 57, 43, 56, 60, 39, 52, 58],\n",
      "        [46, 53, 59,  1, 41, 39, 52, 57],\n",
      "        [50, 58,  1, 40, 43,  1, 53, 52],\n",
      "        [46,  1, 40, 63,  1, 51, 63,  1],\n",
      "        [56, 43, 57, 53, 50, 60, 43, 42],\n",
      "        [ 1, 42, 47, 42,  0, 30, 59, 52],\n",
      "        [58, 58, 43, 56,  1, 57, 43, 52],\n",
      "        [42, 43, 56,  6,  1, 63, 53, 59],\n",
      "        [50,  1, 39, 57,  1, 50, 53, 52],\n",
      "        [50, 53, 61, 43, 56, 57,  1, 58],\n",
      "        [39, 56,  6,  1, 61, 46, 53,  1],\n",
      "        [53, 60, 43,  1, 44, 50, 39, 58],\n",
      "        [39, 57,  1, 52, 53, 58,  1, 58],\n",
      "        [ 7, 47, 52,  1, 58, 46, 43,  1],\n",
      "        [31, 47, 41, 47, 50, 47, 39,  6],\n",
      "        [43, 47, 56,  1, 53, 44, 44, 47],\n",
      "        [11,  0, 31, 59, 54, 54, 43, 56],\n",
      "        [39, 57, 43, 42,  1, 63, 53, 59],\n",
      "        [ 1, 53,  5, 43, 56,  1, 53, 59],\n",
      "        [39, 58, 46, 43, 56,  6,  1, 58]])\n",
      "targets:\n",
      "torch.Size([32, 8])\n",
      "tensor([[56, 39, 47, 52,  0, 32, 46, 43],\n",
      "        [51, 43, 56,  1, 51, 39, 50, 39],\n",
      "        [31, 32, 25, 27, 30, 17, 24, 13],\n",
      "        [16, 10,  0, 32, 46, 43,  1, 61],\n",
      "        [32, 46, 43,  1, 54, 59, 56, 47],\n",
      "        [ 1, 41, 46, 53, 49, 43,  1, 63],\n",
      "        [26, 32, 21, 27, 10,  0, 31, 46],\n",
      "        [63,  1, 57, 39, 63, 47, 52, 45],\n",
      "        [ 1, 44, 39, 50, 50,  1, 59, 54],\n",
      "        [ 1, 43, 56, 43,  1, 63, 53, 59],\n",
      "        [ 1, 43, 60, 43, 56, 63,  1, 53],\n",
      "        [57, 43, 52, 42, 47, 52, 45,  1],\n",
      "        [57, 43, 56, 60, 39, 52, 58, 57],\n",
      "        [53, 59,  1, 41, 39, 52, 57, 58],\n",
      "        [58,  1, 40, 43,  1, 53, 52,  1],\n",
      "        [ 1, 40, 63,  1, 51, 63,  1, 54],\n",
      "        [43, 57, 53, 50, 60, 43, 42,  0],\n",
      "        [42, 47, 42,  0, 30, 59, 52,  1],\n",
      "        [58, 43, 56,  1, 57, 43, 52, 58],\n",
      "        [43, 56,  6,  1, 63, 53, 59, 56],\n",
      "        [ 1, 39, 57,  1, 50, 53, 52, 45],\n",
      "        [53, 61, 43, 56, 57,  1, 58, 53],\n",
      "        [56,  6,  1, 61, 46, 53,  1, 46],\n",
      "        [60, 43,  1, 44, 50, 39, 58, 58],\n",
      "        [57,  1, 52, 53, 58,  1, 58, 46],\n",
      "        [47, 52,  1, 58, 46, 43,  1, 54],\n",
      "        [47, 41, 47, 50, 47, 39,  6,  0],\n",
      "        [47, 56,  1, 53, 44, 44, 47, 41],\n",
      "        [ 0, 31, 59, 54, 54, 43, 56,  1],\n",
      "        [57, 43, 42,  1, 63, 53, 59,  1],\n",
      "        [53,  5, 43, 56,  1, 53, 59, 56],\n",
      "        [58, 46, 43, 56,  6,  1, 58, 53]])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train') #comment\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['emma',\n",
    " 'olivia',\n",
    " 'ava',\n",
    " 'isabella',\n",
    " 'sophia',\n",
    " 'charlotte',\n",
    " 'mia',\n",
    " 'amelia',\n",
    " 'harper',\n",
    " 'evelyn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e m\n",
      "m m\n",
      "m a\n"
     ]
    }
   ],
   "source": [
    "for w in words[:1]:\n",
    "  for ch1, ch2 in zip(w, w[1:]):\n",
    "    print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dictionary.get(keyname, value)` where keyname is The keyname of the item you want to return the value from and value is A value to return if the specified key does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('h', 'e'): 1}\n"
     ]
    }
   ],
   "source": [
    "b = {}\n",
    "bigram = ('h', 'e')\n",
    "\n",
    "# First encounter of ('h', 'e')\n",
    "count = b.get(bigram, 0)  # Since ('h', 'e') is not in b, this returns 0\n",
    "b[bigram] = count + 1     # Sets b[('h', 'e')] to 1\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b = {}\n",
    "for w in words:\n",
    "  chs = ['<S>'] + list(w) + ['<E>']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    bigram = (ch1, ch2)\n",
    "    b[bigram] = b.get(bigram, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('a', '<E>'), 7),\n",
       " (('i', 'a'), 4),\n",
       " (('e', 'l'), 3),\n",
       " (('<S>', 'e'), 2),\n",
       " (('l', 'i'), 2),\n",
       " (('<S>', 'a'), 2),\n",
       " (('h', 'a'), 2),\n",
       " (('a', 'r'), 2),\n",
       " (('e', 'm'), 1),\n",
       " (('m', 'm'), 1),\n",
       " (('m', 'a'), 1),\n",
       " (('<S>', 'o'), 1),\n",
       " (('o', 'l'), 1),\n",
       " (('i', 'v'), 1),\n",
       " (('v', 'i'), 1),\n",
       " (('a', 'v'), 1),\n",
       " (('v', 'a'), 1),\n",
       " (('<S>', 'i'), 1),\n",
       " (('i', 's'), 1),\n",
       " (('s', 'a'), 1),\n",
       " (('a', 'b'), 1),\n",
       " (('b', 'e'), 1),\n",
       " (('l', 'l'), 1),\n",
       " (('l', 'a'), 1),\n",
       " (('<S>', 's'), 1),\n",
       " (('s', 'o'), 1),\n",
       " (('o', 'p'), 1),\n",
       " (('p', 'h'), 1),\n",
       " (('h', 'i'), 1),\n",
       " (('<S>', 'c'), 1),\n",
       " (('c', 'h'), 1),\n",
       " (('r', 'l'), 1),\n",
       " (('l', 'o'), 1),\n",
       " (('o', 't'), 1),\n",
       " (('t', 't'), 1),\n",
       " (('t', 'e'), 1),\n",
       " (('e', '<E>'), 1),\n",
       " (('<S>', 'm'), 1),\n",
       " (('m', 'i'), 1),\n",
       " (('a', 'm'), 1),\n",
       " (('m', 'e'), 1),\n",
       " (('<S>', 'h'), 1),\n",
       " (('r', 'p'), 1),\n",
       " (('p', 'e'), 1),\n",
       " (('e', 'r'), 1),\n",
       " (('r', '<E>'), 1),\n",
       " (('e', 'v'), 1),\n",
       " (('v', 'e'), 1),\n",
       " (('l', 'y'), 1),\n",
       " (('y', 'n'), 1),\n",
       " (('n', '<E>'), 1)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(b.items(), key = lambda kv: -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {\n",
    "  ('h', 'e'): 2,\n",
    "  ('e', 'l'): 1,\n",
    "  ('l', 'l'): 1,\n",
    "  ('l', 'o'): 1,\n",
    "  ('o', '<E>'): 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('h', 'e'), 2),\n",
       " (('e', 'l'), 1),\n",
       " (('l', 'l'), 1),\n",
       " (('l', 'o'), 1),\n",
       " (('o', '<E>'), 1)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(x.items(), key = lambda kv: -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"I\", \"like\", \"cats\", \"dogs\", \"car\", \"shwetha\"]\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input idx:\n",
      " tensor([[4, 4, 0, 1, 3],\n",
      "        [4, 2, 0, 5, 4]])\n",
      "\n",
      "Logits:\n",
      " tensor([[[ 1.3940, -0.5681, -0.5765, -0.0823,  0.9343, -0.6740],\n",
      "         [ 1.3940, -0.5681, -0.5765, -0.0823,  0.9343, -0.6740],\n",
      "         [-0.8982, -0.1593, -0.1351, -1.8574, -1.2355,  0.3585],\n",
      "         [-1.4117, -0.0254, -0.2865, -0.0451,  0.2312, -0.9341],\n",
      "         [ 1.0827, -0.9749, -0.1530,  0.1494,  0.7607,  0.7706]],\n",
      "\n",
      "        [[ 1.3940, -0.5681, -0.5765, -0.0823,  0.9343, -0.6740],\n",
      "         [ 0.1232, -0.8815, -0.0243,  0.6466, -1.0736, -1.4312],\n",
      "         [-0.8982, -0.1593, -0.1351, -1.8574, -1.2355,  0.3585],\n",
      "         [-0.2351,  0.6542, -3.1659,  0.2164,  1.2839,  1.4845],\n",
      "         [ 1.3940, -0.5681, -0.5765, -0.0823,  0.9343, -0.6740]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the vocabulary size and the input tensor\n",
    "vocab_size = len(vocab) # for example, a vocabulary size of 10\n",
    "B, T = 2, 5  # Batch size (B) and sequence length (T)\n",
    "\n",
    "# Initialize the embedding layer (equivalent to self.token_embedding_table)\n",
    "token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "# Define an example input tensor idx of shape (B, T)\n",
    "idx = torch.randint(0, vocab_size, (B, T))  # Random integer values in the range [0, vocab_size)\n",
    "print(\"Input idx:\\n\", idx)\n",
    "\n",
    "# Forward pass\n",
    "logits = token_embedding_table(idx)  # Output shape will be (B, T, C) where C = vocab_size\n",
    "print(\"\\nLogits:\\n\", logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits after embedding lookup: tensor([[[-0.5698, -0.3917, -2.5088, -0.2929, -0.4578],\n",
      "         [-1.7342,  0.2785, -0.7114,  1.6899,  0.5533],\n",
      "         [-0.7548,  0.2590,  1.8964, -0.0588,  0.2411]],\n",
      "\n",
      "        [[-0.7548,  0.2590,  1.8964, -0.0588,  0.2411],\n",
      "         [-0.9542, -0.0894, -1.8531, -0.1106, -0.7076],\n",
      "         [ 0.0239, -1.0283, -0.2174,  1.8099,  1.5296]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Logits shape: torch.Size([2, 3, 5])\n",
      "B, T, C: 2 3 5\n",
      "Logits after reshaping: tensor([[-0.5698, -0.3917, -2.5088, -0.2929, -0.4578],\n",
      "        [-1.7342,  0.2785, -0.7114,  1.6899,  0.5533],\n",
      "        [-0.7548,  0.2590,  1.8964, -0.0588,  0.2411],\n",
      "        [-0.7548,  0.2590,  1.8964, -0.0588,  0.2411],\n",
      "        [-0.9542, -0.0894, -1.8531, -0.1106, -0.7076],\n",
      "        [ 0.0239, -1.0283, -0.2174,  1.8099,  1.5296]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Logits shape after reshaping: torch.Size([6, 5])\n",
      "Targets after reshaping: tensor([1, 2, 3, 3, 4, 0])\n",
      "Targets shape after reshaping: torch.Size([6])\n",
      "Cross-entropy loss: tensor(2.2408, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example parameters\n",
    "B, T, vocab_size = 2, 3, 5  # batch size, sequence length, vocabulary size\n",
    "\n",
    "# Initialize embedding layer manually (usually in __init__ in a class)\n",
    "token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "# Example input tensors\n",
    "idx = torch.tensor([[0, 1, 2], [2, 3, 4]])  # (B, T)\n",
    "targets = torch.tensor([[1, 2, 3], [3, 4, 0]])  # (B, T)\n",
    "\n",
    "# Step 1: Look up logits from the embedding table\n",
    "logits = token_embedding_table(idx)  # Shape will be (B, T, C)\n",
    "print(\"Logits after embedding lookup:\", logits)\n",
    "print(\"Logits shape:\", logits.shape)  # Expected shape: (B, T, vocab_size)\n",
    "\n",
    "# Step 2: Check if targets are provided to calculate loss\n",
    "if targets is None:\n",
    "    loss = None\n",
    "else:\n",
    "    # Step 3: Get shapes of B, T, and C from logits\n",
    "    B, T, C = logits.shape\n",
    "    print(\"B, T, C:\", B, T, C)\n",
    "\n",
    "    # Step 4: Flatten logits and targets for cross-entropy computation\n",
    "    logits = logits.view(B * T, C)\n",
    "    print(\"Logits after reshaping:\", logits)\n",
    "    print(\"Logits shape after reshaping:\", logits.shape)  # Expected shape: (B*T, C)\n",
    "\n",
    "    targets = targets.view(B * T)\n",
    "    print(\"Targets after reshaping:\", targets)\n",
    "    print(\"Targets shape after reshaping:\", targets.shape)  # Expected shape: (B*T,)\n",
    "\n",
    "    # Step 5: Calculate the cross-entropy loss\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "    print(\"Cross-entropy loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`generate` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities after softmax - Dimension = -1: tensor([[0.2117, 0.6345, 0.1538],\n",
      "        [0.2117, 0.6345, 0.1538]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the vocabulary size and initialize the model\n",
    "vocab_size = 3 # Example vocabulary size\n",
    "B, T = 2, 3  # Batch size and sequence length for demonstration\n",
    "max_new_tokens = 5  # Number of new tokens to generate\n",
    "\n",
    "# Starting sequence, a (B, T) tensor of indices\n",
    "idx = torch.randint(0, vocab_size, (B, T))  # Initial random tokens\n",
    "#print(\"Initial idx:\", idx)  # Shape: (B, T)\n",
    "\n",
    "# Simulate the model's token embedding table (simplified for this example)\n",
    "token_embedding_table = torch.nn.Embedding(vocab_size, vocab_size)\n",
    "#print(\"Token Embedding Table:\", token_embedding_table.weight)\n",
    "\n",
    "# Run the generation loop for a specified number of new tokens\n",
    "for _ in range(max_new_tokens):\n",
    "    # Get the predictions by looking up embeddings (simulates the forward pass of the model)\n",
    "    logits = token_embedding_table(idx)  # Shape: (B, T, vocab_size)\n",
    "    #print(\"\\nLogits (output of token embedding):\", logits)\n",
    "\n",
    "    # Focus only on the last time step\n",
    "    logits = logits[:, -1, :]  # Shape: (B, vocab_size)\n",
    "    #print(\"Logits at the last time step:\", logits)\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    probs = F.softmax(logits, dim=-1)  # Shape: (B, vocab_size)\n",
    "    #print(\"Probabilities after softmax - Dimension = -1:\", probs)\n",
    "\n",
    "    # Sample from the distribution\n",
    "    idx_next = torch.multinomial(probs, num_samples=1)  # Shape: (B, 1)\n",
    "    #print(\"Sampled next token indices:\", idx_next)\n",
    "\n",
    "    # Append sampled index to the running sequence\n",
    "    idx = torch.cat((idx, idx_next), dim=1)  # Shape: (B, T+1)\n",
    "    #print(\"Updated idx with new token:\", idx)\n",
    "\n",
    "#print(\"\\nFinal generated sequence:\", idx)\n",
    "print(\"Probabilities after softmax - Dimension = -1:\", probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`logits_last_token = logits[:, -1, :]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original logits shape: torch.Size([3, 5, 5])\n",
      "Original logits: tensor([[[ 0.7197, -0.0312,  0.3210, -0.5477, -0.4302],\n",
      "         [ 0.7591, -1.2154, -0.3034, -0.4355,  0.2557],\n",
      "         [-1.6738,  1.3958,  0.4602,  1.1874, -0.6585],\n",
      "         [-0.2604,  0.6578, -0.6115,  0.8724, -0.7121],\n",
      "         [ 0.7083, -0.5542,  0.8778,  0.0765,  0.4233]],\n",
      "\n",
      "        [[-0.0604,  1.3901,  0.5978, -0.8579,  0.5132],\n",
      "         [ 0.3595,  1.1219,  0.0804,  0.3566,  0.1255],\n",
      "         [ 0.3984, -0.8485, -0.5143,  0.7305, -0.1207],\n",
      "         [ 1.2420,  0.9079,  0.3548,  0.7696, -0.1712],\n",
      "         [-0.4675, -0.8515,  0.0537,  0.7609, -0.6885]],\n",
      "\n",
      "        [[-1.7235,  0.6089,  1.2678,  1.1129,  0.3321],\n",
      "         [-1.3550, -1.3064,  0.0601, -1.8122,  0.6211],\n",
      "         [ 0.4274,  0.1511,  0.9602, -0.5113, -0.4914],\n",
      "         [-0.2934,  0.0562,  0.3438, -1.1166,  0.9537],\n",
      "         [-0.5646, -0.0401, -0.7710,  2.2142, -0.7124]]])\n",
      "\n",
      "Logits after selecting the last token of each sequence:\n",
      "Shape of logits_last_token: torch.Size([3, 5])\n",
      "tensor([[ 0.7083, -0.5542,  0.8778,  0.0765,  0.4233],\n",
      "        [-0.4675, -0.8515,  0.0537,  0.7609, -0.6885],\n",
      "        [-0.5646, -0.0401, -0.7710,  2.2142, -0.7124]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Let's assume we have logits of shape (B, T, C)\n",
    "B, T, C = 3, 5, 5  # 3 sequences, each of length 5, with 10 classes (vocab size)\n",
    "\n",
    "# Random logits for demonstration\n",
    "logits = torch.randn(B, T, C)\n",
    "print(\"Original logits shape:\", logits.shape)\n",
    "print(\"Original logits:\", logits)\n",
    "\n",
    "# Now apply the slicing to focus on the last time step of each sequence\n",
    "logits_last_token = logits[:, -1, :]  # Take the last token (T = 5, last one is T=-1)\n",
    "\n",
    "print(\"\\nLogits after selecting the last token of each sequence:\")\n",
    "print(\"Shape of logits_last_token:\", logits_last_token.shape)\n",
    "print(logits_last_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token indices sampled for each sequence: tensor([[2],\n",
      "        [0],\n",
      "        [0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Sample probabilities for 3 sequences (batch size = 3) with 5 possible tokens (vocabulary size = 5)\n",
    "probs = torch.tensor([[0.1, 0.3, 0.4, 0.1, 0.1],    # probabilities for sequence 1\n",
    "                      [0.2, 0.1, 0.1, 0.3, 0.3],    # probabilities for sequence 2\n",
    "                      [0.3, 0.2, 0.2, 0.1, 0.2]])   # probabilities for sequence 3\n",
    "\n",
    "# Sample the next token (1 token for each sequence)\n",
    "idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "print(\"Next token indices sampled for each sequence:\", idx_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Token', 'ization', 'Ġis', 'Ġfun', '!']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# Use GPT2TokenizerFast directly for compatibility\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "text = \"Tokenization is fun!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Byte-Pair Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karthee\\Desktop\\CSULB\\Project\\LLM Project\\llmvenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m itos \u001b[38;5;241m=\u001b[39m {i: ch \u001b[38;5;28;01mfor\u001b[39;00m i, ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chars)}  \u001b[38;5;66;03m# Integer to string\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# GPT-2 Tokenizer (subword-level encoding)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2TokenizerFast\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Use GPT2TokenizerFast directly for compatibility\u001b[39;00m\n\u001b[0;32m     11\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m GPT2TokenizerFast\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\karthee\\Desktop\\CSULB\\Project\\LLM Project\\llmvenv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1766\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1764\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1766\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1767\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32mc:\\Users\\karthee\\Desktop\\CSULB\\Project\\LLM Project\\llmvenv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1778\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1777\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1778\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1779\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1780\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1781\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1782\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1783\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\karthee\\Desktop\\CSULB\\Project\\LLM Project\\llmvenv\\Lib\\site-packages\\transformers\\models\\__init__.py:15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     albert,\n\u001b[0;32m     17\u001b[0m     align,\n\u001b[0;32m     18\u001b[0m     altclip,\n\u001b[0;32m     19\u001b[0m     audio_spectrogram_transformer,\n\u001b[0;32m     20\u001b[0m     auto,\n\u001b[0;32m     21\u001b[0m     autoformer,\n\u001b[0;32m     22\u001b[0m     bark,\n\u001b[0;32m     23\u001b[0m     bart,\n\u001b[0;32m     24\u001b[0m     barthez,\n\u001b[0;32m     25\u001b[0m     bartpho,\n\u001b[0;32m     26\u001b[0m     beit,\n\u001b[0;32m     27\u001b[0m     bert,\n\u001b[0;32m     28\u001b[0m     bert_generation,\n\u001b[0;32m     29\u001b[0m     bert_japanese,\n\u001b[0;32m     30\u001b[0m     bertweet,\n\u001b[0;32m     31\u001b[0m     big_bird,\n\u001b[0;32m     32\u001b[0m     bigbird_pegasus,\n\u001b[0;32m     33\u001b[0m     biogpt,\n\u001b[0;32m     34\u001b[0m     bit,\n\u001b[0;32m     35\u001b[0m     blenderbot,\n\u001b[0;32m     36\u001b[0m     blenderbot_small,\n\u001b[0;32m     37\u001b[0m     blip,\n\u001b[0;32m     38\u001b[0m     blip_2,\n\u001b[0;32m     39\u001b[0m     bloom,\n\u001b[0;32m     40\u001b[0m     bridgetower,\n\u001b[0;32m     41\u001b[0m     bros,\n\u001b[0;32m     42\u001b[0m     byt5,\n\u001b[0;32m     43\u001b[0m     camembert,\n\u001b[0;32m     44\u001b[0m     canine,\n\u001b[0;32m     45\u001b[0m     chameleon,\n\u001b[0;32m     46\u001b[0m     chinese_clip,\n\u001b[0;32m     47\u001b[0m     clap,\n\u001b[0;32m     48\u001b[0m     clip,\n\u001b[0;32m     49\u001b[0m     clipseg,\n\u001b[0;32m     50\u001b[0m     clvp,\n\u001b[0;32m     51\u001b[0m     code_llama,\n\u001b[0;32m     52\u001b[0m     codegen,\n\u001b[0;32m     53\u001b[0m     cohere,\n\u001b[0;32m     54\u001b[0m     conditional_detr,\n\u001b[0;32m     55\u001b[0m     convbert,\n\u001b[0;32m     56\u001b[0m     convnext,\n\u001b[0;32m     57\u001b[0m     convnextv2,\n\u001b[0;32m     58\u001b[0m     cpm,\n\u001b[0;32m     59\u001b[0m     cpmant,\n\u001b[0;32m     60\u001b[0m     ctrl,\n\u001b[0;32m     61\u001b[0m     cvt,\n\u001b[0;32m     62\u001b[0m     dac,\n\u001b[0;32m     63\u001b[0m     data2vec,\n\u001b[0;32m     64\u001b[0m     dbrx,\n\u001b[0;32m     65\u001b[0m     deberta,\n\u001b[0;32m     66\u001b[0m     deberta_v2,\n\u001b[0;32m     67\u001b[0m     decision_transformer,\n\u001b[0;32m     68\u001b[0m     deformable_detr,\n\u001b[0;32m     69\u001b[0m     deit,\n\u001b[0;32m     70\u001b[0m     deprecated,\n\u001b[0;32m     71\u001b[0m     depth_anything,\n\u001b[0;32m     72\u001b[0m     detr,\n\u001b[0;32m     73\u001b[0m     dialogpt,\n\u001b[0;32m     74\u001b[0m     dinat,\n\u001b[0;32m     75\u001b[0m     dinov2,\n\u001b[0;32m     76\u001b[0m     distilbert,\n\u001b[0;32m     77\u001b[0m     dit,\n\u001b[0;32m     78\u001b[0m     donut,\n\u001b[0;32m     79\u001b[0m     dpr,\n\u001b[0;32m     80\u001b[0m     dpt,\n\u001b[0;32m     81\u001b[0m     efficientnet,\n\u001b[0;32m     82\u001b[0m     electra,\n\u001b[0;32m     83\u001b[0m     encodec,\n\u001b[0;32m     84\u001b[0m     encoder_decoder,\n\u001b[0;32m     85\u001b[0m     ernie,\n\u001b[0;32m     86\u001b[0m     esm,\n\u001b[0;32m     87\u001b[0m     falcon,\n\u001b[0;32m     88\u001b[0m     falcon_mamba,\n\u001b[0;32m     89\u001b[0m     fastspeech2_conformer,\n\u001b[0;32m     90\u001b[0m     flaubert,\n\u001b[0;32m     91\u001b[0m     flava,\n\u001b[0;32m     92\u001b[0m     fnet,\n\u001b[0;32m     93\u001b[0m     focalnet,\n\u001b[0;32m     94\u001b[0m     fsmt,\n\u001b[0;32m     95\u001b[0m     funnel,\n\u001b[0;32m     96\u001b[0m     fuyu,\n\u001b[0;32m     97\u001b[0m     gemma,\n\u001b[0;32m     98\u001b[0m     gemma2,\n\u001b[0;32m     99\u001b[0m     git,\n\u001b[0;32m    100\u001b[0m     glm,\n\u001b[0;32m    101\u001b[0m     glpn,\n\u001b[0;32m    102\u001b[0m     gpt2,\n\u001b[0;32m    103\u001b[0m     gpt_bigcode,\n\u001b[0;32m    104\u001b[0m     gpt_neo,\n\u001b[0;32m    105\u001b[0m     gpt_neox,\n\u001b[0;32m    106\u001b[0m     gpt_neox_japanese,\n\u001b[0;32m    107\u001b[0m     gpt_sw3,\n\u001b[0;32m    108\u001b[0m     gptj,\n\u001b[0;32m    109\u001b[0m     granite,\n\u001b[0;32m    110\u001b[0m     granitemoe,\n\u001b[0;32m    111\u001b[0m     grounding_dino,\n\u001b[0;32m    112\u001b[0m     groupvit,\n\u001b[0;32m    113\u001b[0m     herbert,\n\u001b[0;32m    114\u001b[0m     hiera,\n\u001b[0;32m    115\u001b[0m     hubert,\n\u001b[0;32m    116\u001b[0m     ibert,\n\u001b[0;32m    117\u001b[0m     idefics,\n\u001b[0;32m    118\u001b[0m     idefics2,\n\u001b[0;32m    119\u001b[0m     idefics3,\n\u001b[0;32m    120\u001b[0m     imagegpt,\n\u001b[0;32m    121\u001b[0m     informer,\n\u001b[0;32m    122\u001b[0m     instructblip,\n\u001b[0;32m    123\u001b[0m     instructblipvideo,\n\u001b[0;32m    124\u001b[0m     jamba,\n\u001b[0;32m    125\u001b[0m     jetmoe,\n\u001b[0;32m    126\u001b[0m     kosmos2,\n\u001b[0;32m    127\u001b[0m     layoutlm,\n\u001b[0;32m    128\u001b[0m     layoutlmv2,\n\u001b[0;32m    129\u001b[0m     layoutlmv3,\n\u001b[0;32m    130\u001b[0m     layoutxlm,\n\u001b[0;32m    131\u001b[0m     led,\n\u001b[0;32m    132\u001b[0m     levit,\n\u001b[0;32m    133\u001b[0m     lilt,\n\u001b[0;32m    134\u001b[0m     llama,\n\u001b[0;32m    135\u001b[0m     llava,\n\u001b[0;32m    136\u001b[0m     llava_next,\n\u001b[0;32m    137\u001b[0m     llava_next_video,\n\u001b[0;32m    138\u001b[0m     llava_onevision,\n\u001b[0;32m    139\u001b[0m     longformer,\n\u001b[0;32m    140\u001b[0m     longt5,\n\u001b[0;32m    141\u001b[0m     luke,\n\u001b[0;32m    142\u001b[0m     lxmert,\n\u001b[0;32m    143\u001b[0m     m2m_100,\n\u001b[0;32m    144\u001b[0m     mamba,\n\u001b[0;32m    145\u001b[0m     mamba2,\n\u001b[0;32m    146\u001b[0m     marian,\n\u001b[0;32m    147\u001b[0m     markuplm,\n\u001b[0;32m    148\u001b[0m     mask2former,\n\u001b[0;32m    149\u001b[0m     maskformer,\n\u001b[0;32m    150\u001b[0m     mbart,\n\u001b[0;32m    151\u001b[0m     mbart50,\n\u001b[0;32m    152\u001b[0m     megatron_bert,\n\u001b[0;32m    153\u001b[0m     megatron_gpt2,\n\u001b[0;32m    154\u001b[0m     mgp_str,\n\u001b[0;32m    155\u001b[0m     mimi,\n\u001b[0;32m    156\u001b[0m     mistral,\n\u001b[0;32m    157\u001b[0m     mixtral,\n\u001b[0;32m    158\u001b[0m     mllama,\n\u001b[0;32m    159\u001b[0m     mluke,\n\u001b[0;32m    160\u001b[0m     mobilebert,\n\u001b[0;32m    161\u001b[0m     mobilenet_v1,\n\u001b[0;32m    162\u001b[0m     mobilenet_v2,\n\u001b[0;32m    163\u001b[0m     mobilevit,\n\u001b[0;32m    164\u001b[0m     mobilevitv2,\n\u001b[0;32m    165\u001b[0m     moshi,\n\u001b[0;32m    166\u001b[0m     mpnet,\n\u001b[0;32m    167\u001b[0m     mpt,\n\u001b[0;32m    168\u001b[0m     mra,\n\u001b[0;32m    169\u001b[0m     mt5,\n\u001b[0;32m    170\u001b[0m     musicgen,\n\u001b[0;32m    171\u001b[0m     musicgen_melody,\n\u001b[0;32m    172\u001b[0m     mvp,\n\u001b[0;32m    173\u001b[0m     myt5,\n\u001b[0;32m    174\u001b[0m     nemotron,\n\u001b[0;32m    175\u001b[0m     nllb,\n\u001b[0;32m    176\u001b[0m     nllb_moe,\n\u001b[0;32m    177\u001b[0m     nougat,\n\u001b[0;32m    178\u001b[0m     nystromformer,\n\u001b[0;32m    179\u001b[0m     olmo,\n\u001b[0;32m    180\u001b[0m     olmoe,\n\u001b[0;32m    181\u001b[0m     omdet_turbo,\n\u001b[0;32m    182\u001b[0m     oneformer,\n\u001b[0;32m    183\u001b[0m     openai,\n\u001b[0;32m    184\u001b[0m     opt,\n\u001b[0;32m    185\u001b[0m     owlv2,\n\u001b[0;32m    186\u001b[0m     owlvit,\n\u001b[0;32m    187\u001b[0m     paligemma,\n\u001b[0;32m    188\u001b[0m     patchtsmixer,\n\u001b[0;32m    189\u001b[0m     patchtst,\n\u001b[0;32m    190\u001b[0m     pegasus,\n\u001b[0;32m    191\u001b[0m     pegasus_x,\n\u001b[0;32m    192\u001b[0m     perceiver,\n\u001b[0;32m    193\u001b[0m     persimmon,\n\u001b[0;32m    194\u001b[0m     phi,\n\u001b[0;32m    195\u001b[0m     phi3,\n\u001b[0;32m    196\u001b[0m     phimoe,\n\u001b[0;32m    197\u001b[0m     phobert,\n\u001b[0;32m    198\u001b[0m     pix2struct,\n\u001b[0;32m    199\u001b[0m     pixtral,\n\u001b[0;32m    200\u001b[0m     plbart,\n\u001b[0;32m    201\u001b[0m     poolformer,\n\u001b[0;32m    202\u001b[0m     pop2piano,\n\u001b[0;32m    203\u001b[0m     prophetnet,\n\u001b[0;32m    204\u001b[0m     pvt,\n\u001b[0;32m    205\u001b[0m     pvt_v2,\n\u001b[0;32m    206\u001b[0m     qwen2,\n\u001b[0;32m    207\u001b[0m     qwen2_audio,\n\u001b[0;32m    208\u001b[0m     qwen2_moe,\n\u001b[0;32m    209\u001b[0m     qwen2_vl,\n\u001b[0;32m    210\u001b[0m     rag,\n\u001b[0;32m    211\u001b[0m     recurrent_gemma,\n\u001b[0;32m    212\u001b[0m     reformer,\n\u001b[0;32m    213\u001b[0m     regnet,\n\u001b[0;32m    214\u001b[0m     rembert,\n\u001b[0;32m    215\u001b[0m     resnet,\n\u001b[0;32m    216\u001b[0m     roberta,\n\u001b[0;32m    217\u001b[0m     roberta_prelayernorm,\n\u001b[0;32m    218\u001b[0m     roc_bert,\n\u001b[0;32m    219\u001b[0m     roformer,\n\u001b[0;32m    220\u001b[0m     rt_detr,\n\u001b[0;32m    221\u001b[0m     rwkv,\n\u001b[0;32m    222\u001b[0m     sam,\n\u001b[0;32m    223\u001b[0m     seamless_m4t,\n\u001b[0;32m    224\u001b[0m     seamless_m4t_v2,\n\u001b[0;32m    225\u001b[0m     segformer,\n\u001b[0;32m    226\u001b[0m     seggpt,\n\u001b[0;32m    227\u001b[0m     sew,\n\u001b[0;32m    228\u001b[0m     sew_d,\n\u001b[0;32m    229\u001b[0m     siglip,\n\u001b[0;32m    230\u001b[0m     speech_encoder_decoder,\n\u001b[0;32m    231\u001b[0m     speech_to_text,\n\u001b[0;32m    232\u001b[0m     speecht5,\n\u001b[0;32m    233\u001b[0m     splinter,\n\u001b[0;32m    234\u001b[0m     squeezebert,\n\u001b[0;32m    235\u001b[0m     stablelm,\n\u001b[0;32m    236\u001b[0m     starcoder2,\n\u001b[0;32m    237\u001b[0m     superpoint,\n\u001b[0;32m    238\u001b[0m     swiftformer,\n\u001b[0;32m    239\u001b[0m     swin,\n\u001b[0;32m    240\u001b[0m     swin2sr,\n\u001b[0;32m    241\u001b[0m     swinv2,\n\u001b[0;32m    242\u001b[0m     switch_transformers,\n\u001b[0;32m    243\u001b[0m     t5,\n\u001b[0;32m    244\u001b[0m     table_transformer,\n\u001b[0;32m    245\u001b[0m     tapas,\n\u001b[0;32m    246\u001b[0m     time_series_transformer,\n\u001b[0;32m    247\u001b[0m     timesformer,\n\u001b[0;32m    248\u001b[0m     timm_backbone,\n\u001b[0;32m    249\u001b[0m     trocr,\n\u001b[0;32m    250\u001b[0m     tvp,\n\u001b[0;32m    251\u001b[0m     udop,\n\u001b[0;32m    252\u001b[0m     umt5,\n\u001b[0;32m    253\u001b[0m     unispeech,\n\u001b[0;32m    254\u001b[0m     unispeech_sat,\n\u001b[0;32m    255\u001b[0m     univnet,\n\u001b[0;32m    256\u001b[0m     upernet,\n\u001b[0;32m    257\u001b[0m     video_llava,\n\u001b[0;32m    258\u001b[0m     videomae,\n\u001b[0;32m    259\u001b[0m     vilt,\n\u001b[0;32m    260\u001b[0m     vipllava,\n\u001b[0;32m    261\u001b[0m     vision_encoder_decoder,\n\u001b[0;32m    262\u001b[0m     vision_text_dual_encoder,\n\u001b[0;32m    263\u001b[0m     visual_bert,\n\u001b[0;32m    264\u001b[0m     vit,\n\u001b[0;32m    265\u001b[0m     vit_mae,\n\u001b[0;32m    266\u001b[0m     vit_msn,\n\u001b[0;32m    267\u001b[0m     vitdet,\n\u001b[0;32m    268\u001b[0m     vitmatte,\n\u001b[0;32m    269\u001b[0m     vits,\n\u001b[0;32m    270\u001b[0m     vivit,\n\u001b[0;32m    271\u001b[0m     wav2vec2,\n\u001b[0;32m    272\u001b[0m     wav2vec2_bert,\n\u001b[0;32m    273\u001b[0m     wav2vec2_conformer,\n\u001b[0;32m    274\u001b[0m     wav2vec2_phoneme,\n\u001b[0;32m    275\u001b[0m     wav2vec2_with_lm,\n\u001b[0;32m    276\u001b[0m     wavlm,\n\u001b[0;32m    277\u001b[0m     whisper,\n\u001b[0;32m    278\u001b[0m     x_clip,\n\u001b[0;32m    279\u001b[0m     xglm,\n\u001b[0;32m    280\u001b[0m     xlm,\n\u001b[0;32m    281\u001b[0m     xlm_roberta,\n\u001b[0;32m    282\u001b[0m     xlm_roberta_xl,\n\u001b[0;32m    283\u001b[0m     xlnet,\n\u001b[0;32m    284\u001b[0m     xmod,\n\u001b[0;32m    285\u001b[0m     yolos,\n\u001b[0;32m    286\u001b[0m     yoso,\n\u001b[0;32m    287\u001b[0m     zamba,\n\u001b[0;32m    288\u001b[0m     zoedepth,\n\u001b[0;32m    289\u001b[0m )\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:991\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1087\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1186\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text = \"I love you shwetha. I miss you so much\"\n",
    "\n",
    "chars = sorted(set(text))  # Unique characters in the text\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}  # String to integer\n",
    "itos = {i: ch for i, ch in enumerate(chars)}  # Integer to string\n",
    "\n",
    "# GPT-2 Tokenizer (subword-level encoding)\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# Use GPT2TokenizerFast directly for compatibility\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokens = tokenizer.tokenize(text)\n",
    "encoded_tokens = tokenizer.encode(text)\n",
    "decoded_text = tokenizer.decode(encoded_tokens)\n",
    "\n",
    "print(\"Subword tokens:\", tokens)\n",
    "print(\"Subword encoded:\", encoded_tokens)\n",
    "print(\"Subword decoded:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subword tokens: ['I', 'Ġlove', 'Ġyou', 'Ġsh', 'w', 'eth', 'a', '.', 'ĠI', 'Ġmiss', 'Ġyou', 'Ġso', 'Ġmuch']\n",
      "Subword encoded: [40, 1842, 345, 427, 86, 2788, 64, 13, 314, 2051, 345, 523, 881]\n",
      "Subword decoded: I love you shwetha. I miss you so much\n"
     ]
    }
   ],
   "source": [
    "text = \"I love you shwetha. I miss you so much\"\n",
    "\n",
    "# GPT-2 Tokenizer (subword-level encoding)\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# Use GPT2TokenizerFast directly for compatibility\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokens = tokenizer.tokenize(text)\n",
    "encoded_tokens = tokenizer.encode(text)\n",
    "decoded_text = tokenizer.decode(encoded_tokens)\n",
    "\n",
    "print(\"Subword tokens:\", tokens)\n",
    "print(\"Subword encoded:\", encoded_tokens)\n",
    "print(\"Subword decoded:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character names and dialogues extracted and saved to HP_OP.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_dialogues_with_annotations(script_path, output_path):\n",
    "    \"\"\"\n",
    "    Extracts character names and their dialogues from a movie script,\n",
    "    handling annotations like (CONT'D) or (O.S.) and encoding issues.\n",
    "\n",
    "    Args:\n",
    "        script_path (str): Path to the input movie script file.\n",
    "        output_path (str): Path to save the output file with character names and dialogues.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the file with UTF-8 encoding\n",
    "        with open(script_path, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "        \n",
    "        dialogues = []\n",
    "        # Regex to match character names with optional annotations\n",
    "        character_pattern = re.compile(r\"^[A-Z][A-Z\\s]*(\\(\\w+.*\\))?$\")\n",
    "\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line = lines[i].strip()\n",
    "\n",
    "            # Check if the line matches a character name (with optional annotations)\n",
    "            if character_pattern.match(line):\n",
    "                # Remove annotations (e.g., \"(CONT'D)\", \"(O.S.)\") from the character name\n",
    "                character_name = re.sub(r\"\\s*\\(.*?\\)\", \"\", line).strip()\n",
    "                \n",
    "                # Gather all subsequent non-empty lines as dialogues\n",
    "                j = i + 1\n",
    "                dialogue_lines = []\n",
    "                while j < len(lines) and not character_pattern.match(lines[j].strip()) and lines[j].strip():\n",
    "                    dialogue_lines.append(lines[j].strip())\n",
    "                    j += 1\n",
    "                if dialogue_lines:\n",
    "                    dialogues.append(f\"{character_name}:\\n\" + \" \".join(dialogue_lines) + \"\\n\")\n",
    "                i = j - 1  # Adjust the index to continue from where we left off\n",
    "            i += 1\n",
    "\n",
    "        # Write extracted character names and dialogues to the output file with UTF-8 encoding\n",
    "        with open(output_path, 'w', encoding='utf-8') as file:\n",
    "            for dialogue in dialogues:\n",
    "                file.write(dialogue + \"\\n\")\n",
    "        \n",
    "        print(f\"Character names and dialogues extracted and saved to {output_path}\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: The script file path is invalid.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage\n",
    "script_file = \"input3.txt\"  # Input script file\n",
    "output_file = \"HP_OP.txt\"  # Output file with cleaned character names and dialogues\n",
    "\n",
    "extract_dialogues_with_annotations(script_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data written to Cleaned_Test.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean the content\n",
    "def clean_file(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        data = infile.read()\n",
    "    \n",
    "    # Updated regex to handle both single and double quotes\n",
    "    pattern = r'\\{\\s*[\"\\']question[\"\\']\\s*:\\s*[\"\\'](.*?)[\"\\'],\\s*[\"\\']answer[\"\\']\\s*:\\s*[\"\\'](.*?)[\"\\'],.*?\\}'\n",
    "    matches = re.findall(pattern, data, re.DOTALL)\n",
    "\n",
    "    cleaned_data = []\n",
    "    for question, answer in matches:\n",
    "        # Remove extra spaces and normalize whitespace\n",
    "        question = re.sub(r'\\s+', ' ', question).strip()\n",
    "        answer = re.sub(r'\\s+', ' ', answer).strip()\n",
    "        cleaned_data.append(f\"QUESTION: {question}\\nANSWER:{answer}\\n\\n\")\n",
    "    \n",
    "    # Write cleaned data to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        outfile.writelines(cleaned_data)\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = 'ip.txt'  # Replace with your input file name\n",
    "output_file = 'Cleaned_Test.txt'\n",
    "\n",
    "# Clean the file\n",
    "clean_file(input_file, output_file)\n",
    "print(f\"Cleaned data written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BOOK to DIALOGUE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import chardet\n",
    "\n",
    "# Input and output file paths\n",
    "input_file_path = \"input3.txt\"  # Replace with the actual file name\n",
    "output_file_path = \"harry_potter_dialogues.txt\"\n",
    "\n",
    "# Characters to track and identify as speakers\n",
    "characters = [\"Harry\", \"Ron\", \"Hermione\", \"Hagrid\", \"Dumbledore\", \"Snape\", \"Mr. Dursley\", \"Mrs. Dursley\"]\n",
    "\n",
    "# Detect file encoding\n",
    "with open(input_file_path, \"rb\") as file:\n",
    "    result = chardet.detect(file.read())\n",
    "    file_encoding = result['encoding']\n",
    "\n",
    "# Regex patterns to detect dialogues and speaker cues\n",
    "dialogue_pattern = re.compile(r'“([^”]+)”')\n",
    "speaker_pattern = re.compile(r'\\b(' + '|'.join(characters) + r')\\b\\s*(said|asked|replied|yelled|whispered|cried|muttered|exclaimed|answered|shouted)', re.IGNORECASE)\n",
    "\n",
    "# Function to clean up dialogue lines\n",
    "def clean_dialogue(line):\n",
    "    return line.replace('“', '').replace('”', '').strip()\n",
    "\n",
    "# Track the last identified speaker\n",
    "last_speaker = None\n",
    "\n",
    "# Open the input text file for reading with detected encoding\n",
    "with open(input_file_path, \"r\", encoding=file_encoding) as input_file:\n",
    "    # Open the output file to write the dialogues\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        # Combine consecutive lines if needed\n",
    "        combined_line = \"\"\n",
    "        \n",
    "        # Iterate through each line in the text file\n",
    "        for line in input_file:\n",
    "            # Remove leading/trailing whitespace\n",
    "            line = line.strip()\n",
    "\n",
    "            # If the line is empty, process the current combined line\n",
    "            if not line and combined_line:\n",
    "                # Check if the line contains dialogue and speaker info\n",
    "                dialogue_match = dialogue_pattern.findall(combined_line)\n",
    "                speaker_match = speaker_pattern.search(combined_line)\n",
    "\n",
    "                if dialogue_match:\n",
    "                    # If there's a speaker match, update the last known speaker\n",
    "                    if speaker_match:\n",
    "                        last_speaker = speaker_match.group(1)\n",
    "\n",
    "                    # Write each dialogue snippet with the identified or last speaker\n",
    "                    for dialogue in dialogue_match:\n",
    "                        if last_speaker:\n",
    "                            output_file.write(f\"{last_speaker}:\\n{clean_dialogue(dialogue)}\\n\\n\")\n",
    "                        else:\n",
    "                            output_file.write(f\"Unknown:\\n{clean_dialogue(dialogue)}\\n\\n\")\n",
    "\n",
    "                # Clear the combined line after processing\n",
    "                combined_line = \"\"\n",
    "                continue\n",
    "\n",
    "            # Append the current line to the combined line\n",
    "            if combined_line:\n",
    "                combined_line += \" \" + line\n",
    "            else:\n",
    "                combined_line = line\n",
    "\n",
    "        # Check any remaining combined line after the file ends\n",
    "        if combined_line:\n",
    "            dialogue_match = dialogue_pattern.findall(combined_line)\n",
    "            speaker_match = speaker_pattern.search(combined_line)\n",
    "\n",
    "            if dialogue_match:\n",
    "                if speaker_match:\n",
    "                    last_speaker = speaker_match.group(1)\n",
    "\n",
    "                for dialogue in dialogue_match:\n",
    "                    if last_speaker:\n",
    "                        output_file.write(f\"{last_speaker}:\\n{clean_dialogue(dialogue)}\\n\\n\")\n",
    "                    else:\n",
    "                        output_file.write(f\"Unknown:\\n{clean_dialogue(dialogue)}\\n\\n\")\n",
    "\n",
    "print(f\"Dialogues have been successfully extracted to '{output_file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special characters removed. Cleaned data saved to Harry_Potter_cleaned.txt.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define a function to remove special characters\n",
    "def remove_special_characters(text):\n",
    "    # Keep only letters, digits, and basic punctuation (optional)\n",
    "    cleaned_text = re.sub(r'[^A-Za-z0-9\\s.,?!:;\\'\"]+', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "# File paths\n",
    "input_file_path = 'Harry_Potter.txt'  # Input file\n",
    "output_file_path = 'Harry_Potter_cleaned.txt'  # Output file\n",
    "\n",
    "# Read the input file\n",
    "with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Remove special characters\n",
    "cleaned_data = remove_special_characters(data)\n",
    "\n",
    "# Write the cleaned data to a new file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(cleaned_data)\n",
    "\n",
    "print(f\"Special characters removed. Cleaned data saved to {output_file_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cleaned and saved to: output.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_bracketed_text(file_path, output_file_path=None):\n",
    "    \"\"\"\n",
    "    Removes all occurrences of text enclosed in square brackets (e.g., [1]) from the file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the input file.\n",
    "        output_file_path (str, optional): Path to save the cleaned file. If not provided, overwrites the input file.\n",
    "    \"\"\"\n",
    "    # Read the file content\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Remove text enclosed in square brackets\n",
    "    cleaned_content = re.sub(r'\\[.*?\\]', '', content)\n",
    "    \n",
    "    # Determine the output path\n",
    "    output_file_path = output_file_path or file_path\n",
    "    \n",
    "    # Write the cleaned content back to the file\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(cleaned_content)\n",
    "    \n",
    "    print(f\"File cleaned and saved to: {output_file_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_file = 'Cleaned_Text.txt'  # Replace with your file path\n",
    "output_file = 'output.txt'  # Optional: Replace with desired output file path or omit for overwrite\n",
    "remove_bracketed_text(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters removed and file saved as output.txt\n"
     ]
    }
   ],
   "source": [
    "# Specify the input and output file paths\n",
    "input_file = \"input1.txt\"  # Replace with your file name\n",
    "output_file = \"output.txt\"\n",
    "\n",
    "# Characters to remove\n",
    "chars_to_remove = \"<>?div\"\n",
    "\n",
    "# Open the input file with the correct encoding\n",
    "try:\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "except UnicodeDecodeError:\n",
    "    print(\"Error reading the file. Try checking its encoding format.\")\n",
    "\n",
    "# Remove the specified characters from the content\n",
    "filtered_content = ''.join(c for c in content if c not in chars_to_remove)\n",
    "\n",
    "# Write the filtered content to the output file\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(filtered_content)\n",
    "\n",
    "print(\"Characters removed and file saved as\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text written to output.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# File paths\n",
    "input_file = 'input1.txt'  # Replace with your input file name\n",
    "output_file = 'output.txt'  # Replace with your output file name\n",
    "\n",
    "# Read the input file\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Use regex to remove all occurrences of sequences like &#8221;\n",
    "cleaned_text = re.sub(r'&#\\d+;', '', text)\n",
    "\n",
    "# Write the cleaned text to the output file\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    file.write(cleaned_text)\n",
    "\n",
    "print(f\"Cleaned text written to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
