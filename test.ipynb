{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot (330).png](<attachment:Screenshot (330).png>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '\\n')\n",
      "(1, ' ')\n",
      "(2, '!')\n",
      "(3, '$')\n",
      "(4, '&')\n",
      "(5, \"'\")\n",
      "(6, ',')\n",
      "(7, '-')\n",
      "(8, '.')\n",
      "(9, '3')\n",
      "(10, ':')\n",
      "(11, ';')\n",
      "(12, '?')\n",
      "(13, 'A')\n",
      "(14, 'B')\n",
      "(15, 'C')\n",
      "(16, 'D')\n",
      "(17, 'E')\n",
      "(18, 'F')\n",
      "(19, 'G')\n",
      "(20, 'H')\n",
      "(21, 'I')\n",
      "(22, 'J')\n",
      "(23, 'K')\n",
      "(24, 'L')\n",
      "(25, 'M')\n",
      "(26, 'N')\n",
      "(27, 'O')\n",
      "(28, 'P')\n",
      "(29, 'Q')\n",
      "(30, 'R')\n",
      "(31, 'S')\n",
      "(32, 'T')\n",
      "(33, 'U')\n",
      "(34, 'V')\n",
      "(35, 'W')\n",
      "(36, 'X')\n",
      "(37, 'Y')\n",
      "(38, 'Z')\n",
      "(39, 'a')\n",
      "(40, 'b')\n",
      "(41, 'c')\n",
      "(42, 'd')\n",
      "(43, 'e')\n",
      "(44, 'f')\n",
      "(45, 'g')\n",
      "(46, 'h')\n",
      "(47, 'i')\n",
      "(48, 'j')\n",
      "(49, 'k')\n",
      "(50, 'l')\n",
      "(51, 'm')\n",
      "(52, 'n')\n",
      "(53, 'o')\n",
      "(54, 'p')\n",
      "(55, 'q')\n",
      "(56, 'r')\n",
      "(57, 's')\n",
      "(58, 't')\n",
      "(59, 'u')\n",
      "(60, 'v')\n",
      "(61, 'w')\n",
      "(62, 'x')\n",
      "(63, 'y')\n",
      "(64, 'z')\n"
     ]
    }
   ],
   "source": [
    "for i in enumerate(chars):\n",
    "    print(i) #it will return a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(21, 'I'), (28, 'P'), (50, 'l'), (8, '.'), (51, 'm'), (5, \"'\"), (20, 'H'), (42, 'd'), (62, 'x'), (44, 'f'), (30, 'R'), (35, 'W'), (7, '-'), (29, 'Q'), (27, 'O'), (59, 'u'), (3, '$'), (34, 'V'), (12, '?'), (1, ' '), (46, 'h'), (58, 't'), (18, 'F'), (48, 'j'), (15, 'C'), (13, 'A'), (22, 'J'), (10, ':'), (57, 's'), (2, '!'), (61, 'w'), (41, 'c'), (63, 'y'), (24, 'L'), (53, 'o'), (56, 'r'), (36, 'X'), (43, 'e'), (47, 'i'), (39, 'a'), (16, 'D'), (52, 'n'), (25, 'M'), (23, 'K'), (32, 'T'), (37, 'Y'), (49, 'k'), (40, 'b'), (11, ';'), (14, 'B'), (60, 'v'), (17, 'E'), (38, 'Z'), (4, '&'), (19, 'G'), (31, 'S'), (54, 'p'), (64, 'z'), (0, '\\n'), (6, ','), (26, 'N'), (45, 'g'), (9, '3'), (55, 'q'), (33, 'U')}\n"
     ]
    }
   ],
   "source": [
    "stoi = { i for i in enumerate(chars) }\n",
    "print(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "print(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"lambda\" functions are anonymous functions. They are useful when you need a small function temporarily and don't want to define a full function with def.** `lambda parameters: expression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "square = lambda x: x ** 2\n",
    "print(square(5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39, 40, 41]\n"
     ]
    }
   ],
   "source": [
    "encode = lambda s: [stoi[c] for c in s] #list comprehension\n",
    "a = encode(\"abc\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39, 40, 41]\n"
     ]
    }
   ],
   "source": [
    "s = 'abc'\n",
    "def encode(s): \n",
    "    a = [] \n",
    "    for c in s:  \n",
    "        a.append(stoi[c])\n",
    "    return a\n",
    "data = encode(s)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PyTorch tensor is a multi-dimensional array or matrix that is used to store data and perform computations in PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0130,  0.4786, -1.5338],\n",
      "        [-0.5835,  1.9606, -2.1773],\n",
      "        [-1.2816, -1.5758, -0.3407]])\n"
     ]
    }
   ],
   "source": [
    "random_tensor = torch.randn(3, 3)  # 3x3 tensor with random values from normal distribution\n",
    "print(random_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([32, 8])\n",
      "tensor([[42, 56, 39, 47, 52,  0, 32, 46],\n",
      "        [56, 51, 43, 56,  1, 51, 39, 50],\n",
      "        [17, 31, 32, 25, 27, 30, 17, 24],\n",
      "        [26, 16, 10,  0, 32, 46, 43,  1],\n",
      "        [ 0, 32, 46, 43,  1, 54, 59, 56],\n",
      "        [42,  1, 41, 46, 53, 49, 43,  1],\n",
      "        [17, 26, 32, 21, 27, 10,  0, 31],\n",
      "        [51, 63,  1, 57, 39, 63, 47, 52],\n",
      "        [59,  1, 44, 39, 50, 50,  1, 59],\n",
      "        [56,  1, 43, 56, 43,  1, 63, 53],\n",
      "        [43,  1, 43, 60, 43, 56, 63,  1],\n",
      "        [ 1, 57, 43, 52, 42, 47, 52, 45],\n",
      "        [ 1, 57, 43, 56, 60, 39, 52, 58],\n",
      "        [46, 53, 59,  1, 41, 39, 52, 57],\n",
      "        [50, 58,  1, 40, 43,  1, 53, 52],\n",
      "        [46,  1, 40, 63,  1, 51, 63,  1],\n",
      "        [56, 43, 57, 53, 50, 60, 43, 42],\n",
      "        [ 1, 42, 47, 42,  0, 30, 59, 52],\n",
      "        [58, 58, 43, 56,  1, 57, 43, 52],\n",
      "        [42, 43, 56,  6,  1, 63, 53, 59],\n",
      "        [50,  1, 39, 57,  1, 50, 53, 52],\n",
      "        [50, 53, 61, 43, 56, 57,  1, 58],\n",
      "        [39, 56,  6,  1, 61, 46, 53,  1],\n",
      "        [53, 60, 43,  1, 44, 50, 39, 58],\n",
      "        [39, 57,  1, 52, 53, 58,  1, 58],\n",
      "        [ 7, 47, 52,  1, 58, 46, 43,  1],\n",
      "        [31, 47, 41, 47, 50, 47, 39,  6],\n",
      "        [43, 47, 56,  1, 53, 44, 44, 47],\n",
      "        [11,  0, 31, 59, 54, 54, 43, 56],\n",
      "        [39, 57, 43, 42,  1, 63, 53, 59],\n",
      "        [ 1, 53,  5, 43, 56,  1, 53, 59],\n",
      "        [39, 58, 46, 43, 56,  6,  1, 58]])\n",
      "targets:\n",
      "torch.Size([32, 8])\n",
      "tensor([[56, 39, 47, 52,  0, 32, 46, 43],\n",
      "        [51, 43, 56,  1, 51, 39, 50, 39],\n",
      "        [31, 32, 25, 27, 30, 17, 24, 13],\n",
      "        [16, 10,  0, 32, 46, 43,  1, 61],\n",
      "        [32, 46, 43,  1, 54, 59, 56, 47],\n",
      "        [ 1, 41, 46, 53, 49, 43,  1, 63],\n",
      "        [26, 32, 21, 27, 10,  0, 31, 46],\n",
      "        [63,  1, 57, 39, 63, 47, 52, 45],\n",
      "        [ 1, 44, 39, 50, 50,  1, 59, 54],\n",
      "        [ 1, 43, 56, 43,  1, 63, 53, 59],\n",
      "        [ 1, 43, 60, 43, 56, 63,  1, 53],\n",
      "        [57, 43, 52, 42, 47, 52, 45,  1],\n",
      "        [57, 43, 56, 60, 39, 52, 58, 57],\n",
      "        [53, 59,  1, 41, 39, 52, 57, 58],\n",
      "        [58,  1, 40, 43,  1, 53, 52,  1],\n",
      "        [ 1, 40, 63,  1, 51, 63,  1, 54],\n",
      "        [43, 57, 53, 50, 60, 43, 42,  0],\n",
      "        [42, 47, 42,  0, 30, 59, 52,  1],\n",
      "        [58, 43, 56,  1, 57, 43, 52, 58],\n",
      "        [43, 56,  6,  1, 63, 53, 59, 56],\n",
      "        [ 1, 39, 57,  1, 50, 53, 52, 45],\n",
      "        [53, 61, 43, 56, 57,  1, 58, 53],\n",
      "        [56,  6,  1, 61, 46, 53,  1, 46],\n",
      "        [60, 43,  1, 44, 50, 39, 58, 58],\n",
      "        [57,  1, 52, 53, 58,  1, 58, 46],\n",
      "        [47, 52,  1, 58, 46, 43,  1, 54],\n",
      "        [47, 41, 47, 50, 47, 39,  6,  0],\n",
      "        [47, 56,  1, 53, 44, 44, 47, 41],\n",
      "        [ 0, 31, 59, 54, 54, 43, 56,  1],\n",
      "        [57, 43, 42,  1, 63, 53, 59,  1],\n",
      "        [53,  5, 43, 56,  1, 53, 59, 56],\n",
      "        [58, 46, 43, 56,  6,  1, 58, 53]])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train') #comment\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['emma',\n",
    " 'olivia',\n",
    " 'ava',\n",
    " 'isabella',\n",
    " 'sophia',\n",
    " 'charlotte',\n",
    " 'mia',\n",
    " 'amelia',\n",
    " 'harper',\n",
    " 'evelyn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e m\n",
      "m m\n",
      "m a\n"
     ]
    }
   ],
   "source": [
    "for w in words[:1]:\n",
    "  for ch1, ch2 in zip(w, w[1:]):\n",
    "    print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dictionary.get(keyname, value)` where keyname is The keyname of the item you want to return the value from and value is A value to return if the specified key does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('h', 'e'): 1}\n"
     ]
    }
   ],
   "source": [
    "b = {}\n",
    "bigram = ('h', 'e')\n",
    "\n",
    "# First encounter of ('h', 'e')\n",
    "count = b.get(bigram, 0)  # Since ('h', 'e') is not in b, this returns 0\n",
    "b[bigram] = count + 1     # Sets b[('h', 'e')] to 1\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b = {}\n",
    "for w in words:\n",
    "  chs = ['<S>'] + list(w) + ['<E>']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    bigram = (ch1, ch2)\n",
    "    b[bigram] = b.get(bigram, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('a', '<E>'), 7),\n",
       " (('i', 'a'), 4),\n",
       " (('e', 'l'), 3),\n",
       " (('<S>', 'e'), 2),\n",
       " (('l', 'i'), 2),\n",
       " (('<S>', 'a'), 2),\n",
       " (('h', 'a'), 2),\n",
       " (('a', 'r'), 2),\n",
       " (('e', 'm'), 1),\n",
       " (('m', 'm'), 1),\n",
       " (('m', 'a'), 1),\n",
       " (('<S>', 'o'), 1),\n",
       " (('o', 'l'), 1),\n",
       " (('i', 'v'), 1),\n",
       " (('v', 'i'), 1),\n",
       " (('a', 'v'), 1),\n",
       " (('v', 'a'), 1),\n",
       " (('<S>', 'i'), 1),\n",
       " (('i', 's'), 1),\n",
       " (('s', 'a'), 1),\n",
       " (('a', 'b'), 1),\n",
       " (('b', 'e'), 1),\n",
       " (('l', 'l'), 1),\n",
       " (('l', 'a'), 1),\n",
       " (('<S>', 's'), 1),\n",
       " (('s', 'o'), 1),\n",
       " (('o', 'p'), 1),\n",
       " (('p', 'h'), 1),\n",
       " (('h', 'i'), 1),\n",
       " (('<S>', 'c'), 1),\n",
       " (('c', 'h'), 1),\n",
       " (('r', 'l'), 1),\n",
       " (('l', 'o'), 1),\n",
       " (('o', 't'), 1),\n",
       " (('t', 't'), 1),\n",
       " (('t', 'e'), 1),\n",
       " (('e', '<E>'), 1),\n",
       " (('<S>', 'm'), 1),\n",
       " (('m', 'i'), 1),\n",
       " (('a', 'm'), 1),\n",
       " (('m', 'e'), 1),\n",
       " (('<S>', 'h'), 1),\n",
       " (('r', 'p'), 1),\n",
       " (('p', 'e'), 1),\n",
       " (('e', 'r'), 1),\n",
       " (('r', '<E>'), 1),\n",
       " (('e', 'v'), 1),\n",
       " (('v', 'e'), 1),\n",
       " (('l', 'y'), 1),\n",
       " (('y', 'n'), 1),\n",
       " (('n', '<E>'), 1)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(b.items(), key = lambda kv: -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {\n",
    "  ('h', 'e'): 2,\n",
    "  ('e', 'l'): 1,\n",
    "  ('l', 'l'): 1,\n",
    "  ('l', 'o'): 1,\n",
    "  ('o', '<E>'): 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('h', 'e'), 2),\n",
       " (('e', 'l'), 1),\n",
       " (('l', 'l'), 1),\n",
       " (('l', 'o'), 1),\n",
       " (('o', '<E>'), 1)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(x.items(), key = lambda kv: -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"I\", \"like\", \"cats\", \"dogs\", \"car\", \"shwetha\"]\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input idx:\n",
      " tensor([[4, 4, 0, 1, 3],\n",
      "        [4, 2, 0, 5, 4]])\n",
      "\n",
      "Logits:\n",
      " tensor([[[ 1.3940, -0.5681, -0.5765, -0.0823,  0.9343, -0.6740],\n",
      "         [ 1.3940, -0.5681, -0.5765, -0.0823,  0.9343, -0.6740],\n",
      "         [-0.8982, -0.1593, -0.1351, -1.8574, -1.2355,  0.3585],\n",
      "         [-1.4117, -0.0254, -0.2865, -0.0451,  0.2312, -0.9341],\n",
      "         [ 1.0827, -0.9749, -0.1530,  0.1494,  0.7607,  0.7706]],\n",
      "\n",
      "        [[ 1.3940, -0.5681, -0.5765, -0.0823,  0.9343, -0.6740],\n",
      "         [ 0.1232, -0.8815, -0.0243,  0.6466, -1.0736, -1.4312],\n",
      "         [-0.8982, -0.1593, -0.1351, -1.8574, -1.2355,  0.3585],\n",
      "         [-0.2351,  0.6542, -3.1659,  0.2164,  1.2839,  1.4845],\n",
      "         [ 1.3940, -0.5681, -0.5765, -0.0823,  0.9343, -0.6740]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the vocabulary size and the input tensor\n",
    "vocab_size = len(vocab) # for example, a vocabulary size of 10\n",
    "B, T = 2, 5  # Batch size (B) and sequence length (T)\n",
    "\n",
    "# Initialize the embedding layer (equivalent to self.token_embedding_table)\n",
    "token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "# Define an example input tensor idx of shape (B, T)\n",
    "idx = torch.randint(0, vocab_size, (B, T))  # Random integer values in the range [0, vocab_size)\n",
    "print(\"Input idx:\\n\", idx)\n",
    "\n",
    "# Forward pass\n",
    "logits = token_embedding_table(idx)  # Output shape will be (B, T, C) where C = vocab_size\n",
    "print(\"\\nLogits:\\n\", logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits after embedding lookup: tensor([[[-0.5698, -0.3917, -2.5088, -0.2929, -0.4578],\n",
      "         [-1.7342,  0.2785, -0.7114,  1.6899,  0.5533],\n",
      "         [-0.7548,  0.2590,  1.8964, -0.0588,  0.2411]],\n",
      "\n",
      "        [[-0.7548,  0.2590,  1.8964, -0.0588,  0.2411],\n",
      "         [-0.9542, -0.0894, -1.8531, -0.1106, -0.7076],\n",
      "         [ 0.0239, -1.0283, -0.2174,  1.8099,  1.5296]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Logits shape: torch.Size([2, 3, 5])\n",
      "B, T, C: 2 3 5\n",
      "Logits after reshaping: tensor([[-0.5698, -0.3917, -2.5088, -0.2929, -0.4578],\n",
      "        [-1.7342,  0.2785, -0.7114,  1.6899,  0.5533],\n",
      "        [-0.7548,  0.2590,  1.8964, -0.0588,  0.2411],\n",
      "        [-0.7548,  0.2590,  1.8964, -0.0588,  0.2411],\n",
      "        [-0.9542, -0.0894, -1.8531, -0.1106, -0.7076],\n",
      "        [ 0.0239, -1.0283, -0.2174,  1.8099,  1.5296]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Logits shape after reshaping: torch.Size([6, 5])\n",
      "Targets after reshaping: tensor([1, 2, 3, 3, 4, 0])\n",
      "Targets shape after reshaping: torch.Size([6])\n",
      "Cross-entropy loss: tensor(2.2408, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example parameters\n",
    "B, T, vocab_size = 2, 3, 5  # batch size, sequence length, vocabulary size\n",
    "\n",
    "# Initialize embedding layer manually (usually in __init__ in a class)\n",
    "token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "# Example input tensors\n",
    "idx = torch.tensor([[0, 1, 2], [2, 3, 4]])  # (B, T)\n",
    "targets = torch.tensor([[1, 2, 3], [3, 4, 0]])  # (B, T)\n",
    "\n",
    "# Step 1: Look up logits from the embedding table\n",
    "logits = token_embedding_table(idx)  # Shape will be (B, T, C)\n",
    "print(\"Logits after embedding lookup:\", logits)\n",
    "print(\"Logits shape:\", logits.shape)  # Expected shape: (B, T, vocab_size)\n",
    "\n",
    "# Step 2: Check if targets are provided to calculate loss\n",
    "if targets is None:\n",
    "    loss = None\n",
    "else:\n",
    "    # Step 3: Get shapes of B, T, and C from logits\n",
    "    B, T, C = logits.shape\n",
    "    print(\"B, T, C:\", B, T, C)\n",
    "\n",
    "    # Step 4: Flatten logits and targets for cross-entropy computation\n",
    "    logits = logits.view(B * T, C)\n",
    "    print(\"Logits after reshaping:\", logits)\n",
    "    print(\"Logits shape after reshaping:\", logits.shape)  # Expected shape: (B*T, C)\n",
    "\n",
    "    targets = targets.view(B * T)\n",
    "    print(\"Targets after reshaping:\", targets)\n",
    "    print(\"Targets shape after reshaping:\", targets.shape)  # Expected shape: (B*T,)\n",
    "\n",
    "    # Step 5: Calculate the cross-entropy loss\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "    print(\"Cross-entropy loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`generate` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities after softmax - Dimension = -1: tensor([[0.2117, 0.6345, 0.1538],\n",
      "        [0.2117, 0.6345, 0.1538]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the vocabulary size and initialize the model\n",
    "vocab_size = 3 # Example vocabulary size\n",
    "B, T = 2, 3  # Batch size and sequence length for demonstration\n",
    "max_new_tokens = 5  # Number of new tokens to generate\n",
    "\n",
    "# Starting sequence, a (B, T) tensor of indices\n",
    "idx = torch.randint(0, vocab_size, (B, T))  # Initial random tokens\n",
    "#print(\"Initial idx:\", idx)  # Shape: (B, T)\n",
    "\n",
    "# Simulate the model's token embedding table (simplified for this example)\n",
    "token_embedding_table = torch.nn.Embedding(vocab_size, vocab_size)\n",
    "#print(\"Token Embedding Table:\", token_embedding_table.weight)\n",
    "\n",
    "# Run the generation loop for a specified number of new tokens\n",
    "for _ in range(max_new_tokens):\n",
    "    # Get the predictions by looking up embeddings (simulates the forward pass of the model)\n",
    "    logits = token_embedding_table(idx)  # Shape: (B, T, vocab_size)\n",
    "    #print(\"\\nLogits (output of token embedding):\", logits)\n",
    "\n",
    "    # Focus only on the last time step\n",
    "    logits = logits[:, -1, :]  # Shape: (B, vocab_size)\n",
    "    #print(\"Logits at the last time step:\", logits)\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    probs = F.softmax(logits, dim=-1)  # Shape: (B, vocab_size)\n",
    "    #print(\"Probabilities after softmax - Dimension = -1:\", probs)\n",
    "\n",
    "    # Sample from the distribution\n",
    "    idx_next = torch.multinomial(probs, num_samples=1)  # Shape: (B, 1)\n",
    "    #print(\"Sampled next token indices:\", idx_next)\n",
    "\n",
    "    # Append sampled index to the running sequence\n",
    "    idx = torch.cat((idx, idx_next), dim=1)  # Shape: (B, T+1)\n",
    "    #print(\"Updated idx with new token:\", idx)\n",
    "\n",
    "#print(\"\\nFinal generated sequence:\", idx)\n",
    "print(\"Probabilities after softmax - Dimension = -1:\", probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`logits_last_token = logits[:, -1, :]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original logits shape: torch.Size([3, 5, 5])\n",
      "Original logits: tensor([[[ 0.7197, -0.0312,  0.3210, -0.5477, -0.4302],\n",
      "         [ 0.7591, -1.2154, -0.3034, -0.4355,  0.2557],\n",
      "         [-1.6738,  1.3958,  0.4602,  1.1874, -0.6585],\n",
      "         [-0.2604,  0.6578, -0.6115,  0.8724, -0.7121],\n",
      "         [ 0.7083, -0.5542,  0.8778,  0.0765,  0.4233]],\n",
      "\n",
      "        [[-0.0604,  1.3901,  0.5978, -0.8579,  0.5132],\n",
      "         [ 0.3595,  1.1219,  0.0804,  0.3566,  0.1255],\n",
      "         [ 0.3984, -0.8485, -0.5143,  0.7305, -0.1207],\n",
      "         [ 1.2420,  0.9079,  0.3548,  0.7696, -0.1712],\n",
      "         [-0.4675, -0.8515,  0.0537,  0.7609, -0.6885]],\n",
      "\n",
      "        [[-1.7235,  0.6089,  1.2678,  1.1129,  0.3321],\n",
      "         [-1.3550, -1.3064,  0.0601, -1.8122,  0.6211],\n",
      "         [ 0.4274,  0.1511,  0.9602, -0.5113, -0.4914],\n",
      "         [-0.2934,  0.0562,  0.3438, -1.1166,  0.9537],\n",
      "         [-0.5646, -0.0401, -0.7710,  2.2142, -0.7124]]])\n",
      "\n",
      "Logits after selecting the last token of each sequence:\n",
      "Shape of logits_last_token: torch.Size([3, 5])\n",
      "tensor([[ 0.7083, -0.5542,  0.8778,  0.0765,  0.4233],\n",
      "        [-0.4675, -0.8515,  0.0537,  0.7609, -0.6885],\n",
      "        [-0.5646, -0.0401, -0.7710,  2.2142, -0.7124]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Let's assume we have logits of shape (B, T, C)\n",
    "B, T, C = 3, 5, 5  # 3 sequences, each of length 5, with 10 classes (vocab size)\n",
    "\n",
    "# Random logits for demonstration\n",
    "logits = torch.randn(B, T, C)\n",
    "print(\"Original logits shape:\", logits.shape)\n",
    "print(\"Original logits:\", logits)\n",
    "\n",
    "# Now apply the slicing to focus on the last time step of each sequence\n",
    "logits_last_token = logits[:, -1, :]  # Take the last token (T = 5, last one is T=-1)\n",
    "\n",
    "print(\"\\nLogits after selecting the last token of each sequence:\")\n",
    "print(\"Shape of logits_last_token:\", logits_last_token.shape)\n",
    "print(logits_last_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token indices sampled for each sequence: tensor([[2],\n",
      "        [0],\n",
      "        [0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Sample probabilities for 3 sequences (batch size = 3) with 5 possible tokens (vocabulary size = 5)\n",
    "probs = torch.tensor([[0.1, 0.3, 0.4, 0.1, 0.1],    # probabilities for sequence 1\n",
    "                      [0.2, 0.1, 0.1, 0.3, 0.3],    # probabilities for sequence 2\n",
    "                      [0.3, 0.2, 0.2, 0.1, 0.2]])   # probabilities for sequence 3\n",
    "\n",
    "# Sample the next token (1 token for each sequence)\n",
    "idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "print(\"Next token indices sampled for each sequence:\", idx_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Token', 'ization', 'Ġis', 'Ġfun', '!']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# Use GPT2TokenizerFast directly for compatibility\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "text = \"Tokenization is fun!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Byte-Pair Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I love you shwetha. I miss you so much\"\n",
    "\n",
    "chars = sorted(set(text))  # Unique characters in the text\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}  # String to integer\n",
    "itos = {i: ch for i, ch in enumerate(chars)}  # Integer to string\n",
    "\n",
    "# GPT-2 Tokenizer (subword-level encoding)\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# Use GPT2TokenizerFast directly for compatibility\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokens = tokenizer.tokenize(text)\n",
    "encoded_tokens = tokenizer.encode(text)\n",
    "decoded_text = tokenizer.decode(encoded_tokens)\n",
    "\n",
    "print(\"Subword tokens:\", tokens)\n",
    "print(\"Subword encoded:\", encoded_tokens)\n",
    "print(\"Subword decoded:\", decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
