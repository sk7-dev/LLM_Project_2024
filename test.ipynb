{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot (330).png](<attachment:Screenshot (330).png>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '\\n')\n",
      "(1, ' ')\n",
      "(2, '!')\n",
      "(3, '$')\n",
      "(4, '&')\n",
      "(5, \"'\")\n",
      "(6, ',')\n",
      "(7, '-')\n",
      "(8, '.')\n",
      "(9, '3')\n",
      "(10, ':')\n",
      "(11, ';')\n",
      "(12, '?')\n",
      "(13, 'A')\n",
      "(14, 'B')\n",
      "(15, 'C')\n",
      "(16, 'D')\n",
      "(17, 'E')\n",
      "(18, 'F')\n",
      "(19, 'G')\n",
      "(20, 'H')\n",
      "(21, 'I')\n",
      "(22, 'J')\n",
      "(23, 'K')\n",
      "(24, 'L')\n",
      "(25, 'M')\n",
      "(26, 'N')\n",
      "(27, 'O')\n",
      "(28, 'P')\n",
      "(29, 'Q')\n",
      "(30, 'R')\n",
      "(31, 'S')\n",
      "(32, 'T')\n",
      "(33, 'U')\n",
      "(34, 'V')\n",
      "(35, 'W')\n",
      "(36, 'X')\n",
      "(37, 'Y')\n",
      "(38, 'Z')\n",
      "(39, 'a')\n",
      "(40, 'b')\n",
      "(41, 'c')\n",
      "(42, 'd')\n",
      "(43, 'e')\n",
      "(44, 'f')\n",
      "(45, 'g')\n",
      "(46, 'h')\n",
      "(47, 'i')\n",
      "(48, 'j')\n",
      "(49, 'k')\n",
      "(50, 'l')\n",
      "(51, 'm')\n",
      "(52, 'n')\n",
      "(53, 'o')\n",
      "(54, 'p')\n",
      "(55, 'q')\n",
      "(56, 'r')\n",
      "(57, 's')\n",
      "(58, 't')\n",
      "(59, 'u')\n",
      "(60, 'v')\n",
      "(61, 'w')\n",
      "(62, 'x')\n",
      "(63, 'y')\n",
      "(64, 'z')\n"
     ]
    }
   ],
   "source": [
    "for i in enumerate(chars):\n",
    "    print(i) #it will return a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(28, 'P'), (25, 'M'), (30, 'R'), (0, '\\n'), (43, 'e'), (45, 'g'), (33, 'U'), (56, 'r'), (14, 'B'), (21, 'I'), (62, 'x'), (19, 'G'), (20, 'H'), (41, 'c'), (27, 'O'), (11, ';'), (55, 'q'), (63, 'y'), (32, 'T'), (5, \"'\"), (40, 'b'), (61, 'w'), (48, 'j'), (17, 'E'), (60, 'v'), (24, 'L'), (64, 'z'), (29, 'Q'), (39, 'a'), (52, 'n'), (36, 'X'), (49, 'k'), (42, 'd'), (7, '-'), (6, ','), (13, 'A'), (57, 's'), (1, ' '), (26, 'N'), (38, 'Z'), (23, 'K'), (15, 'C'), (35, 'W'), (53, 'o'), (46, 'h'), (51, 'm'), (9, '3'), (50, 'l'), (34, 'V'), (59, 'u'), (10, ':'), (54, 'p'), (18, 'F'), (47, 'i'), (31, 'S'), (44, 'f'), (12, '?'), (8, '.'), (4, '&'), (37, 'Y'), (2, '!'), (22, 'J'), (58, 't'), (16, 'D'), (3, '$')}\n"
     ]
    }
   ],
   "source": [
    "stoi = { i for i in enumerate(chars) }\n",
    "print(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "print(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"lambda\" functions are anonymous functions. They are useful when you need a small function temporarily and don't want to define a full function with def.** `lambda parameters: expression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "square = lambda x: x ** 2\n",
    "print(square(5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39, 40, 41]\n"
     ]
    }
   ],
   "source": [
    "encode = lambda s: [stoi[c] for c in s] #list comprehension\n",
    "a = encode(\"abc\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39, 40, 41]\n"
     ]
    }
   ],
   "source": [
    "s = 'abc'\n",
    "def encode(s): \n",
    "    a = [] \n",
    "    for c in s:  \n",
    "        a.append(stoi[c])\n",
    "    return a\n",
    "data = encode(s)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PyTorch tensor is a multi-dimensional array or matrix that is used to store data and perform computations in PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karthee\\Desktop\\CSULB\\Project\\LLM Project\\llmvenv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:295: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.4681e-01, -1.2937e-01, -3.8864e-01],\n",
      "        [ 7.2117e-01, -1.1362e+00, -4.3022e-01],\n",
      "        [-3.9224e-01,  3.6845e-04,  3.9896e-01]])\n"
     ]
    }
   ],
   "source": [
    "random_tensor = torch.randn(3, 3)  # 3x3 tensor with random values from normal distribution\n",
    "print(random_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([32, 8])\n",
      "tensor([[ 0, 31, 43, 41, 53, 52, 42,  1],\n",
      "        [53, 42,  1, 61, 47, 50, 50,  1],\n",
      "        [40, 50, 53, 53, 42,  1, 57, 46],\n",
      "        [27, 10,  0, 15, 53, 51, 43,  1],\n",
      "        [43,  1, 41, 53, 51, 54, 39, 52],\n",
      "        [43,  0, 32, 46, 39, 58,  1, 63],\n",
      "        [ 1, 57, 59, 56, 60, 43, 63,  1],\n",
      "        [52, 47, 45, 46, 58,  8,  0, 19],\n",
      "        [26, 17, 10,  0, 32, 46, 39, 58],\n",
      "        [ 1, 57, 61, 53, 56, 42,  6,  0],\n",
      "        [57, 11,  0, 24, 43, 58,  1, 52],\n",
      "        [54, 56, 53, 59, 42,  1, 14, 53],\n",
      "        [58, 46, 43, 56, 46, 53, 53, 42],\n",
      "        [39,  1, 61, 53, 51, 39, 52,  1],\n",
      "        [47, 60, 43,  1, 42, 47, 56, 43],\n",
      "        [57, 43, 10,  0, 25, 39, 56, 56],\n",
      "        [42,  1, 46, 47, 57,  1, 46, 47],\n",
      "        [31, 32, 17, 30, 10,  0, 15, 59],\n",
      "        [58, 46, 43,  1, 57, 39, 51, 43],\n",
      "        [47, 57,  1, 58, 56, 47, 42, 43],\n",
      "        [35, 46, 39, 58,  1, 51, 59, 57],\n",
      "        [50, 43,  1, 58, 46, 47, 52, 49],\n",
      "        [17, 24, 21, 38, 13, 14, 17, 32],\n",
      "        [54, 53, 52,  1, 46, 47, 57,  1],\n",
      "        [43,  1, 40, 53, 63,  2,  0, 32],\n",
      "        [ 1, 46, 53, 59, 56,  8,  0,  0],\n",
      "        [46, 53, 59, 57, 43,  1, 58, 53],\n",
      "        [44, 59, 56, 63,  1, 58, 53,  1],\n",
      "        [27, 44,  1, 58, 46, 43, 47, 56],\n",
      "        [ 8,  0, 35, 46, 43, 56, 43,  1],\n",
      "        [41, 47, 57, 41, 39, 52,  1, 44],\n",
      "        [44, 53, 53, 50,  6,  1, 47, 58]])\n",
      "targets:\n",
      "torch.Size([32, 8])\n",
      "tensor([[31, 43, 41, 53, 52, 42,  1, 31],\n",
      "        [42,  1, 61, 47, 50, 50,  1, 56],\n",
      "        [50, 53, 53, 42,  1, 57, 46, 53],\n",
      "        [10,  0, 15, 53, 51, 43,  1, 46],\n",
      "        [ 1, 41, 53, 51, 54, 39, 52, 63],\n",
      "        [ 0, 32, 46, 39, 58,  1, 63, 53],\n",
      "        [57, 59, 56, 60, 43, 63,  1, 53],\n",
      "        [47, 45, 46, 58,  8,  0, 19, 47],\n",
      "        [17, 10,  0, 32, 46, 39, 58,  5],\n",
      "        [57, 61, 53, 56, 42,  6,  0, 27],\n",
      "        [11,  0, 24, 43, 58,  1, 52, 53],\n",
      "        [56, 53, 59, 42,  1, 14, 53, 50],\n",
      "        [46, 43, 56, 46, 53, 53, 42,  1],\n",
      "        [ 1, 61, 53, 51, 39, 52,  1, 53],\n",
      "        [60, 43,  1, 42, 47, 56, 43, 41],\n",
      "        [43, 10,  0, 25, 39, 56, 56, 63],\n",
      "        [ 1, 46, 47, 57,  1, 46, 47, 45],\n",
      "        [32, 17, 30, 10,  0, 15, 59, 56],\n",
      "        [46, 43,  1, 57, 39, 51, 43,  1],\n",
      "        [57,  1, 58, 56, 47, 42, 43, 52],\n",
      "        [46, 39, 58,  1, 51, 59, 57, 58],\n",
      "        [43,  1, 58, 46, 47, 52, 49, 57],\n",
      "        [24, 21, 38, 13, 14, 17, 32, 20],\n",
      "        [53, 52,  1, 46, 47, 57,  1, 57],\n",
      "        [ 1, 40, 53, 63,  2,  0, 32, 46],\n",
      "        [46, 53, 59, 56,  8,  0,  0, 15],\n",
      "        [53, 59, 57, 43,  1, 58, 53,  7],\n",
      "        [59, 56, 63,  1, 58, 53,  1, 58],\n",
      "        [44,  1, 58, 46, 43, 47, 56,  1],\n",
      "        [ 0, 35, 46, 43, 56, 43,  1, 47],\n",
      "        [47, 57, 41, 39, 52,  1, 44, 56],\n",
      "        [53, 53, 50,  6,  1, 47, 58,  1]])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train') #comment\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['emma',\n",
    " 'olivia',\n",
    " 'ava',\n",
    " 'isabella',\n",
    " 'sophia',\n",
    " 'charlotte',\n",
    " 'mia',\n",
    " 'amelia',\n",
    " 'harper',\n",
    " 'evelyn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma mma\n",
      "e m\n",
      "m m\n",
      "m a\n"
     ]
    }
   ],
   "source": [
    "for w in words[:1]:\n",
    "  for ch1, ch2 in zip(w, w[1:]):\n",
    "    print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dictionary.get(keyname, value)` where keyname is The keyname of the item you want to return the value from and value is A value to return if the specified key does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('h', 'e'): 1}\n"
     ]
    }
   ],
   "source": [
    "b = {}\n",
    "bigram = ('h', 'e')\n",
    "\n",
    "# First encounter of ('h', 'e')\n",
    "count = b.get(bigram, 0)  # Since ('h', 'e') is not in b, this returns 0\n",
    "b[bigram] = count + 1     # Sets b[('h', 'e')] to 1\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b = {}\n",
    "for w in words:\n",
    "  chs = ['<S>'] + list(w) + ['<E>']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    bigram = (ch1, ch2)\n",
    "    b[bigram] = b.get(bigram, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('a', '<E>'), 7),\n",
       " (('i', 'a'), 4),\n",
       " (('e', 'l'), 3),\n",
       " (('<S>', 'e'), 2),\n",
       " (('l', 'i'), 2),\n",
       " (('<S>', 'a'), 2),\n",
       " (('h', 'a'), 2),\n",
       " (('a', 'r'), 2),\n",
       " (('e', 'm'), 1),\n",
       " (('m', 'm'), 1),\n",
       " (('m', 'a'), 1),\n",
       " (('<S>', 'o'), 1),\n",
       " (('o', 'l'), 1),\n",
       " (('i', 'v'), 1),\n",
       " (('v', 'i'), 1),\n",
       " (('a', 'v'), 1),\n",
       " (('v', 'a'), 1),\n",
       " (('<S>', 'i'), 1),\n",
       " (('i', 's'), 1),\n",
       " (('s', 'a'), 1),\n",
       " (('a', 'b'), 1),\n",
       " (('b', 'e'), 1),\n",
       " (('l', 'l'), 1),\n",
       " (('l', 'a'), 1),\n",
       " (('<S>', 's'), 1),\n",
       " (('s', 'o'), 1),\n",
       " (('o', 'p'), 1),\n",
       " (('p', 'h'), 1),\n",
       " (('h', 'i'), 1),\n",
       " (('<S>', 'c'), 1),\n",
       " (('c', 'h'), 1),\n",
       " (('r', 'l'), 1),\n",
       " (('l', 'o'), 1),\n",
       " (('o', 't'), 1),\n",
       " (('t', 't'), 1),\n",
       " (('t', 'e'), 1),\n",
       " (('e', '<E>'), 1),\n",
       " (('<S>', 'm'), 1),\n",
       " (('m', 'i'), 1),\n",
       " (('a', 'm'), 1),\n",
       " (('m', 'e'), 1),\n",
       " (('<S>', 'h'), 1),\n",
       " (('r', 'p'), 1),\n",
       " (('p', 'e'), 1),\n",
       " (('e', 'r'), 1),\n",
       " (('r', '<E>'), 1),\n",
       " (('e', 'v'), 1),\n",
       " (('v', 'e'), 1),\n",
       " (('l', 'y'), 1),\n",
       " (('y', 'n'), 1),\n",
       " (('n', '<E>'), 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(b.items(), key = lambda kv: -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {\n",
    "  ('h', 'e'): 2,\n",
    "  ('e', 'l'): 1,\n",
    "  ('l', 'l'): 1,\n",
    "  ('l', 'o'): 1,\n",
    "  ('o', '<E>'): 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('h', 'e'), 2),\n",
       " (('e', 'l'), 1),\n",
       " (('l', 'l'), 1),\n",
       " (('l', 'o'), 1),\n",
       " (('o', '<E>'), 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(x.items(), key = lambda kv: -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"I\", \"like\", \"cats\", \"dogs\", \"car\", \"shwetha\"]\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input idx:\n",
      " tensor([[4, 1, 2, 3, 0],\n",
      "        [2, 0, 0, 2, 0]])\n",
      "\n",
      "Logits:\n",
      " tensor([[[ 1.8711,  0.1476, -1.7316,  0.7606,  0.2657, -0.4565],\n",
      "         [-0.1004,  0.4948,  0.3476, -0.2692,  1.1903, -0.5918],\n",
      "         [-0.5214, -0.3283, -1.4382, -0.3248,  0.7840,  0.0599],\n",
      "         [-0.1173, -1.9010,  1.1298,  1.1451,  1.2490,  1.1445],\n",
      "         [-2.0947, -0.7771,  0.4254,  0.3367,  1.6039,  0.1028]],\n",
      "\n",
      "        [[-0.5214, -0.3283, -1.4382, -0.3248,  0.7840,  0.0599],\n",
      "         [-2.0947, -0.7771,  0.4254,  0.3367,  1.6039,  0.1028],\n",
      "         [-2.0947, -0.7771,  0.4254,  0.3367,  1.6039,  0.1028],\n",
      "         [-0.5214, -0.3283, -1.4382, -0.3248,  0.7840,  0.0599],\n",
      "         [-2.0947, -0.7771,  0.4254,  0.3367,  1.6039,  0.1028]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the vocabulary size and the input tensor\n",
    "vocab_size = len(vocab) # for example, a vocabulary size of 10\n",
    "B, T = 2, 5  # Batch size (B) and sequence length (T)\n",
    "\n",
    "# Initialize the embedding layer (equivalent to self.token_embedding_table)\n",
    "token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "# Define an example input tensor idx of shape (B, T)\n",
    "idx = torch.randint(0, vocab_size, (B, T))  # Random integer values in the range [0, vocab_size)\n",
    "print(\"Input idx:\\n\", idx)\n",
    "\n",
    "# Forward pass\n",
    "logits = token_embedding_table(idx)  # Output shape will be (B, T, C) where C = vocab_size\n",
    "print(\"\\nLogits:\\n\", logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits after embedding lookup: tensor([[[ 1.7898,  1.0097, -1.1372, -0.5363,  0.2781],\n",
      "         [-0.4441,  0.1241,  1.0052,  0.2069, -0.2315],\n",
      "         [ 0.3399, -2.1582, -0.7063, -0.6971,  0.8108]],\n",
      "\n",
      "        [[ 0.3399, -2.1582, -0.7063, -0.6971,  0.8108],\n",
      "         [ 0.6533,  0.5064, -0.9853, -0.4036,  1.5754],\n",
      "         [ 0.9056, -1.2434,  0.4143, -2.5756,  0.4180]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Logits shape: torch.Size([2, 3, 5])\n",
      "B, T, C: 2 3 5\n",
      "Logits after reshaping: tensor([[ 1.7898,  1.0097, -1.1372, -0.5363,  0.2781],\n",
      "        [-0.4441,  0.1241,  1.0052,  0.2069, -0.2315],\n",
      "        [ 0.3399, -2.1582, -0.7063, -0.6971,  0.8108],\n",
      "        [ 0.3399, -2.1582, -0.7063, -0.6971,  0.8108],\n",
      "        [ 0.6533,  0.5064, -0.9853, -0.4036,  1.5754],\n",
      "        [ 0.9056, -1.2434,  0.4143, -2.5756,  0.4180]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Logits shape after reshaping: torch.Size([6, 5])\n",
      "Targets after reshaping: tensor([1, 2, 3, 3, 4, 0])\n",
      "Targets shape after reshaping: torch.Size([6])\n",
      "Cross-entropy loss: tensor(1.3844, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example parameters\n",
    "B, T, vocab_size = 2, 3, 5  # batch size, sequence length, vocabulary size\n",
    "\n",
    "# Initialize embedding layer manually (usually in __init__ in a class)\n",
    "token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "# Example input tensors\n",
    "idx = torch.tensor([[0, 1, 2], [2, 3, 4]])  # (B, T)\n",
    "targets = torch.tensor([[1, 2, 3], [3, 4, 0]])  # (B, T)\n",
    "\n",
    "# Step 1: Look up logits from the embedding table\n",
    "logits = token_embedding_table(idx)  # Shape will be (B, T, C)\n",
    "print(\"Logits after embedding lookup:\", logits)\n",
    "print(\"Logits shape:\", logits.shape)  # Expected shape: (B, T, vocab_size)\n",
    "\n",
    "# Step 2: Check if targets are provided to calculate loss\n",
    "if targets is None:\n",
    "    loss = None\n",
    "else:\n",
    "    # Step 3: Get shapes of B, T, and C from logits\n",
    "    B, T, C = logits.shape\n",
    "    print(\"B, T, C:\", B, T, C)\n",
    "\n",
    "    # Step 4: Flatten logits and targets for cross-entropy computation\n",
    "    logits = logits.view(B * T, C)\n",
    "    print(\"Logits after reshaping:\", logits)\n",
    "    print(\"Logits shape after reshaping:\", logits.shape)  # Expected shape: (B*T, C)\n",
    "\n",
    "    targets = targets.view(B * T)\n",
    "    print(\"Targets after reshaping:\", targets)\n",
    "    print(\"Targets shape after reshaping:\", targets.shape)  # Expected shape: (B*T,)\n",
    "\n",
    "    # Step 5: Calculate the cross-entropy loss\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "    print(\"Cross-entropy loss:\", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
